---
layout: post
title: "Vision-Language Navigation (VLN) 综述"
date:   2026-01-22
tags: [VLN, VLA, Robotics, Computer Vision, Deep Learning]
comments: true
author: Tingde Liu
toc: true
excerpt: "视觉语言导航（VLN）是计算机视觉、自然语言处理和机器人导航交叉领域的前沿研究方向。本文对VLN的基本概念、任务类型、主要挑战和最新进展进行全面综述。"
---

# 引言

视觉语言导航（Vision-Language Navigation, VLN）是一个多学科交叉的研究领域，涵盖了自然语言处理、计算机视觉、多模态信息融合以及机器人导航等多个学科。在该领域，研究人员致力于开发能够理解自然语言指令，并在复杂环境中实现自主导航的智能体。

VLN任务的核心挑战在于如何让机器人或智能体理解人类的自然语言指令，并通过视觉感知在真实或虚拟环境中进行导航。这项技术在服务机器人、自动驾驶、智能家居等领域有着广泛的应用前景。

本博文旨在系统梳理VLN领域的研究进展，为学习和研究VLN提供参考。

# VLN基本概述

## 什么是VLN？

VLN任务的定义是：给定一个自然语言指令（natural language instruction），智能体（agent）被放置在模拟器中的初始位置，需要通过理解指令并观察视觉环境，按照指令给定的路线移动到目的地。

这个任务最早起源于2018年，作者认为让一个5岁左右的孩子去拿一个勺子（"bring me a spoon"）是一个很简单的任务，但是如果想通过语言指令去指导机器人去拿一个勺子却非常困难。

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/WX20250824-184006.png" width="80%" />
<figcaption>
VLN任务示意图
</figcaption>
</div>

### VLN 在具身智能体中的角色

近年来，VLN 越来越多地被视为通用 Vision–Language–Action 智能体的一项核心能力，而非孤立的导航任务。在此背景下，VLN 常作为复杂具身任务（如任务执行或协作导航）的子模块进行研究。

*相关任务*：TEACh 等具身任务执行基准

## VLN的三个核心要素

一个完整的VLN系统包含三个核心要素：

1. **Instruction Source（指令源 / Oracle）**：指令源用于生成或提供自然语言导航指令，模拟人类用户对导航目标的描述。在部分交互式 VLN 设定中，智能体可向指令源请求额外信息或澄清指令，从而形成更接近真实人机交互的导航过程。

2. **Agent（智能体/执行者）**：Agent 是 VLN 系统的核心执行主体，负责感知环境、理解语言指令并输出导航动作。智能体需要根据当前视觉观察、历史状态以及语言指令，与环境进行连续交互，完成从起点到目标位置的导航任务。

3. **Environment（环境）**：Environment 定义了智能体执行导航任务的空间。由于真实环境中的数据采集与训练成本较高，现有研究通常依赖高保真模拟器进行训练与评测。例如，在 Room-to-Room（R2R）任务中，Matterport3D 数据集被广泛用作室内三维仿真环境。

## VLN 系统的基本组成

随着具身智能的发展，VLN 系统正从单纯的“指令匹配”向 **VLA (Vision-Language-Action)** 全能模型演进。其架构通常由感知、大脑、行动三个核心模块组成，并呈现出明显的**分层控制（Hierarchical Control）**趋势：

#### 1. 感知模块 (Perception Module) —— 从单一特征到语义-几何融合
* **功能**：负责从环境中提取基于视觉和语言的观察信息。
* **趋势**：从传统的视觉骨干网络（如 CNN）转向**视觉-语言对齐的 Transformer**（如 SigLIP），以获取更强的指令对齐能力；同时结合具有强空间先验的**几何表示模型**（如 DINOv2），以提高在复杂环境中的空间感知与操作精度。

#### 2. 大脑模块 (Reasoning Module) —— 慢思考：高层逻辑与战略规划
* **功能**：作为系统的“慢系统”，负责融合多模态输入，进行高级逻辑推理、常识判断与任务规划。
* **交互逻辑**：**低频率输出**。大脑模块（通常基于预训练的 VLM）利用大规模互联网知识进行推理，不需要实时输出电机信号。它以较低的频率输出高层决策指令或环境中的**目标点像素坐标（Goal Point Coordinates）**。
* **优势**：支持零样本（Zero-shot）泛化，能够将复杂的自然语言指令分解为可执行的中间目标。

#### 3. 行动模块 (Action Module) —— 快行动：高频生成与物理控制
* **功能**：作为系统的“快系统”，将大脑模块的决策指令转化为具体的物理动作。
* **交互逻辑**：**高频率执行**。行动模块接收来自大脑的目标点坐标，利用**连续生成建模**（如扩散模型 Diffusion Model）以高频率（如 30Hz+）预测平滑、无碰撞的**运动轨迹（Trajectory）**。
* **控制闭环**：基于生成的轨迹，利用 **MPC（模型预测控制）** 或底层控制器精准驱动电机（如输出扭矩、位移信号），实现平滑且多模态的动作分布建模。


### VLN 在 VLA 范式下的独特研究价值
虽然 VLN 属于 VLA 体系，但它与传统的“基于 VLA 的机械臂控制（Manipulation）”在任务逻辑上有着本质区别：

* **长时序环境建模 (Long-Horizon Exploration)**：机械臂控制通常关注近场操作，视角相对固定；而 VLN 涉及长距离、多房间甚至跨楼层的移动（如 LHPR-VLN），要求智能体在移动中动态维护环境记忆（如拓扑图或语义地图），处理因位移产生的空间迷失风险。
* **指令流与视觉流的“时空动态对齐”**：在机械臂任务中，指令（如“抓起杯子”）与目标通常是静态对应的；而在 VLN 中，指令解析是随着位移**动态演进**的。
* **分层异步协同需求**：这决定了 VLN 必须采用“大脑模块（慢系统）”与“行动模块（快系统）”的异步协作——大脑负责高层语义状态跟踪，并周期性地将抽象指令转化为行动模块所需的**像素级局部目标点**。
* **常识推理与物理约束的博弈**：VLN 的独特之处在于如何利用生成式策略（如扩散模型）将大脑模块可能存在的“语义幻觉”转化为符合物理规律的连续轨迹，并由 **MPC（模型预测控制）** 处理碰撞与环境摩擦。

### VLN 区别于传统机器人导航
VLN 与传统的机器人导航（Navigation Stack，如基于 SLAM 的系统）在核心驱动力上有显著不同：

* **从“几何坐标”转向“语义路标” (Semantic vs. Geometric)**：
    * **传统导航**：依赖预建的高精度几何地图（点云或占据栅格），通过全局坐标（XY 坐标）驱动。
    * **VLN**：智能体通常置于**未见环境（Unseen Environment）**中，必须通过理解自然语言中的“语义地标”（如“走过红色的椅子后左转”）进行在线决策，而非单纯的坐标追踪。
* **从“被动避障”转向“主动常识搜索”**：
    * **传统导航**：主要解决“如何不撞到障碍物并到达 A 点”。
    * **VLN**：要求智能体具备具身常识。当指令提到“去厨房拿咖啡”时，即便厨房不在视野内，系统也能利用 VLM 的常识预测厨房的方位并规划搜索路径，这超出了传统导航栈的范畴。
* **端到端语义理解的集成**：
    * **传统导航**：感知、规划、执行是解耦的模块。
    * **VLN**：在 VLA 范式下，视觉感知与语言指令在“大脑模块”中深度融合，直接影响行动模块生成的轨迹分布，实现了从高层语义到低层物理动作的端到端映射。

---

## VLN 的主要挑战

### 1. 语言–视觉–行动（Language–Vision–Action）的一致性与可控推理

尽管大规模视觉-语言预训练模型在跨模态对齐方面取得了显著进展，但如何在导航过程中实现语言指令、视觉观测与动作决策之间的语义一致性，仍然是 VLN 的核心挑战之一。特别是在基于大语言模型的 VLN 系统中，如何保证高层语义推理结果能够被可靠地转化为可执行的低层导航动作，是当前研究亟需解决的问题。

### 2. 开放世界场景下的泛化能力

VLN 模型通常在有限数量的场景和指令分布上进行训练，但在实际应用中需要面对开放世界环境，包括未见过的空间布局、物体组合以及多样化的自然语言表达。如何提升模型在跨环境、跨任务和跨指令分布下的零样本或少样本泛化能力，是当前 VLN 研究的关键瓶颈之一。

### 3. 分层规划中长期目标与短期决策的协同

VLN 任务天然具有长时序和部分可观测的特性，智能体需要在理解全局导航目标的同时，根据局部观测进行实时决策。近年来引入的大语言模型在高层规划和子目标分解方面展现出强大的能力，但如何在分层架构中实现高层推理与低层控制策略之间的稳定协同，以及在规划失败时进行有效恢复，仍然是一个具有挑战性的问题。

### 4. 长时记忆与环境建模能力

在复杂室内或室外环境中，智能体需要整合跨时间的多次观测以形成对环境的整体理解。如何构建有效的记忆机制和世界模型，以支持长期导航、路径回溯和错误纠正，是提升 VLN 系统鲁棒性和效率的重要方向。

### 5. 面向真实部署的可靠性与安全性

尽管大多数 VLN 方法仍主要在模拟环境中进行评测，但随着研究逐步向真实机器人系统过渡，导航过程中的安全性、稳定性以及对异常情况的应对能力变得尤为重要。如何在保证导航成功率的同时，避免潜在的危险行为，并提升系统决策过程的可解释性，是 VLN 走向真实应用必须面对的问题。

## VLN研究发展趋势

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/2025-vln-research-timeline.png" width="100%" />
<figcaption>
VLN Research Timeline (Refer from "Thinking-VLN")
</figcaption>
</div>

从整体发展脉络来看，VLN 研究经历了从任务驱动模型到具身智能体的重要转变，其研究重心也随之不断演进。

### 1. 早期阶段：任务驱动的多模态建模（2018–2019）

该阶段的研究主要关注如何构建有效的视觉与语言联合表示，并探索基于循环神经网络或早期 Transformer 架构的导航模型，为 VLN 任务奠定了基础方法论。

### 2. 中期阶段：数据集与评测基准扩展（2020–2021）

随着研究的深入，多个更大规模、更复杂的 VLN 数据集和评测基准被提出，推动模型从简单场景向更具挑战性的真实环境分布扩展，并促进了对泛化能力的系统性研究。

### 3. 过渡阶段：大规模预训练模型的引入（2022–2023）

在这一阶段，预训练视觉-语言模型以及大语言模型被引入 VLN 任务，使模型具备更强的语义理解、推理与指令跟随能力。VLN 开始从任务特定模型向通用多模态模型能力迁移。

### 4. 当前阶段：面向具身智能体的统一建模（2024–至今）

最新研究趋势表明，VLN 正逐步被视为通用具身智能体的重要能力之一，而非孤立的导航任务。研究重点从单一任务性能提升，转向统一感知、推理、规划与执行的多模态智能体框架，并探索其在更广泛现实场景中的应用潜力。

## 关键技术方向

### 1. 统一的视觉-语言-行动（Vision-Language-Action, VLA）架构

近年来，VLN 研究逐渐从传统的模块化设计转向端到端的统一建模范式，将视觉感知、语言理解、空间推理与动作决策统一在同一模型中。基于 Transformer 或大语言模型的 VLA 架构能够有效处理长时序决策问题，并提升复杂自然语言指令下的导航性能，成为当前 VLN 研究的重要趋势。

### 2. 大规模预训练与指令对齐

VLN 模型越来越依赖大规模视觉-语言预训练，以获得更强的跨模态语义对齐与通用表示能力。通过在图文、视频-语言及导航轨迹等多源数据上进行预训练，并结合指令微调（Instruction Tuning），模型在未见环境和复杂指令下的泛化能力显著提升。

### 3. 基于大语言模型的推理与规划

引入大语言模型进行高层语义推理和路径规划已成为 VLN 的重要研究方向。大语言模型可用于子目标分解、动作规划以及轨迹评估，将 VLN 从纯感知控制问题扩展为具备推理与决策能力的导航任务，从而提升复杂场景和长路径下的导航成功率。

### 4. 记忆机制与世界模型构建

为应对部分可观测环境和长距离导航挑战，研究者引入显式或隐式记忆机制以及可学习的世界模型，用于累积环境信息并进行跨时间推理。语义地图和视觉记忆模块能够帮助智能体构建对环境的结构化理解，是实现高效 VLN 的关键组成部分。

### 5. 数据驱动的训练范式与学习策略

相比传统强化学习方法，当前 VLN 更倾向于采用模仿学习、离线强化学习及数据驱动的训练策略。通过轨迹重标注、数据增强以及利用大语言模型生成伪示例，能够有效提升样本效率并降低真实环境交互成本。

### 6. 从 VLN 到通用具身智能任务

VLN 正逐步与目标导航、具身问答和任务执行等具身智能任务融合，形成统一的具身指令跟随问题设定。这一趋势推动 VLN 从单一任务向多任务、多模态的通用具身智能体发展。

## 未来研究方向

### 1. 开放世界环境下的泛化能力

未来 VLN 研究需要突破对封闭场景和固定指令分布的依赖，实现对未知环境、未见物体及组合式指令的零样本或少样本泛化能力。

### 2. 面向真实机器人的导航能力

缩小模拟环境与真实世界之间的差距是 VLN 走向实际应用的关键挑战。未来研究需充分考虑传感器噪声、动态环境和连续控制等现实因素，以提升 VLN 系统在真实机器人平台上的可靠性。

### 3. 基于多模态大模型的统一具身智能体

将 VLN 视为多模态大模型的重要能力之一，而非独立任务，是当前的重要发展方向。通过统一感知、推理、规划与执行，构建具备多任务能力的具身基础模型，有望显著提升 VLN 的通用性和扩展性。

### 4. 持续学习与自主探索

未来的 VLN 智能体需要具备持续学习能力，能够在长期运行过程中自主收集经验、更新环境认知并不断优化导航策略，从而适应不断变化的环境和任务需求。

### 5. 自然人机交互与协作导航

支持更自然的人机交互形式，如对话式指令修正、不完整或模糊指令理解，以及人类实时干预，将显著提升 VLN 系统在真实应用场景中的可用性。

### 6. 安全性、可解释性与失败恢复

随着 VLN 系统逐步走向真实部署，其安全性和可解释性问题愈发重要。未来研究需关注导航决策的可解释性、失败检测与自我纠错机制，以保障系统在复杂环境中的安全运行。

# VLN 任务类型

随着研究范式的演进，VLN 的任务划分逐渐从早期基于指令形式的分类，转向以智能体能力需求和交互方式为核心的分类方式。

## 按推理与决策复杂度划分

**1. 指令跟随型 VLN（Instruction-Following VLN）**

该类任务要求智能体根据给定的自然语言指令，在环境中完成从起点到目标位置的导航，通常不涉及显式目标搜索或复杂语义推理。此类任务主要用于评估模型的语言理解能力和基本导航能力。

*代表性数据集*：Room-to-Room（R2R）、Room-for-Room（R4R）

---

**2. 语义推理驱动的 VLN（Reasoning-Oriented VLN）**

该类任务在导航过程中引入目标物体搜索或语义约束，智能体需要将语言指令与环境中的语义信息进行推理匹配，从而完成导航与定位任务。

*代表性数据集*：REVERIE、SOON

---

**3. 长时序与组合式 VLN（Long-Horizon VLN）**

该类任务强调长距离导航和复杂指令组合，要求智能体具备长期规划、记忆和错误恢复能力，是评估 VLN 系统长期决策能力的重要设定。

---

## 按交互方式划分

**1. 非交互式 VLN**

智能体在接收到初始指令后独立完成导航任务，过程中不与用户进行额外交互。这是当前最常见的 VLN 评测设定。

---

**2. 交互式与对话式 VLN**

该类任务允许智能体在导航过程中与用户进行多轮交互，通过提问或反馈不断优化导航目标，更接近真实人机协作场景。

*代表性数据集*：CVDN

---



# VLN的应用场景

## 室内场景

室内VLN主要关注家庭或办公环境内的导航。环境通常较为复杂，包含多个房间和各种家具，对智能体的空间理解能力要求较高。

<div align="center">
  <img src="/images/vln_indoor.png" width="100%" />
<figcaption>
室内VLN示例
</figcaption>
</div>

**应用示例**：
- 家庭服务机器人
- 室内物流配送
- 智能导览系统

## 室外场景

室外VLN面临更大的环境复杂度，需要处理动态障碍物、天气变化等因素。

<div align="center">
  <img src="/images/vln_outdoor.png" width="100%" />
<figcaption>
室外VLN示例
</figcaption>
</div>

**应用示例**：
- 自动驾驶
- 户外服务机器人
- 城市导航系统

## 空中场景

空中VLN涉及无人机等飞行器的导航控制。

<div align="center">
  <img src="/images/vln_aerial.png" width="100%" />
<figcaption>

室外VLN示例
</figcaption>
</div>
**应用示例**：
- 无人机巡检
- 空中搜救
- 航拍导航



# VLN主流数据集

VLN研究依赖高质量的数据集来训练和评估导航模型。以下是VLN领域最具影响力的主流数据集（含最新进展）：

## 指令导向数据集

### R2R (Room-to-Room)

**基本信息：**

- **发布时间**：2018年
- **规模**：10,567张全景图像，90个真实室内环境
- **指令数量**：7,189条路径，每条路径配有3个自然语言指令
- **环境来源**：Matterport3D真实场景扫描

**特点：**

- VLN领域最早且最具影响力的基准数据集
- 采用离散环境表示（连通图），智能体在预定义视点间导航
- 平均路径长度约为10米，涉及3-5个房间
- 指令由人类标注员编写，具有自然语言的多样性和复杂性

**任务目标**：智能体根据自然语言指令，从起点导航到终点位置

---

### R4R (Room-for-Room)

**基本信息：**

- **发布时间**：2019年
- **环境基础**：基于R2R数据集扩展

**特点：**

- 包含更长、更复杂的导航路径
- 更适合测试模型在复杂多房间环境中的导航能力

---

### RxR (Room-across-Room)

**基本信息：**

- **发布时间**：2020年
- **语言支持**：多语言（英语、印地语、泰卢固语）
- **规模**：比R2R更大规模的数据集

**特点：**

- 支持跨语言VLN研究
- 指令更加详细和具体
- 促进了多语言导航模型的发展

---

### VLN-CE (Vision-and-Language Navigation in Continuous Environments)

**基本信息：**

- **发布时间**：2020年起
- **环境**：Habitat模拟器、Matterport3D连续控制
- **任务类型**：连续控制导航

**特点：**

- 将离散导航拓展为连续控制，更贴近真实机器人场景
- 智能体需要执行连续动作而非仅在离散节点间跳转

---

## 目标导向数据集

### REVERIE

**基本信息：**

- **发布时间**：2020年
- **任务类型**：导航 + 物体定位

**特点：**

- 结合视觉语言导航和物体接地（grounding）
- 包含10,466个指令，涉及21,023个物体实例

**任务目标**：导航到目标物体所在位置，并准确识别目标物体

---

### SOON (Semantic Object-Oriented Navigation)

**基本信息：**

- **任务类型**：基于语义的目标导航

**特点：**

- 强调目标物体的语义理解
- 测试模型的零样本泛化能力

---

### LHPR-VLN (Long-Horizon VLN)

**基本信息：**

- **发布时间**：2025年
- **任务类型**：长视距、多阶段目标导航
- **环境**：室内连续环境

**特点：**

- 强调长程规划与任务分解能力
- 任务平均步长约150步，适合长程导航研究

---

## 对话式导航数据集

### CVDN (Cooperative Vision-and-Dialog Navigation)

**基本信息：**

- **发布时间**：2019年
- **任务类型**：多轮对话式导航

**特点：**

- 智能体可以在导航过程中与Oracle进行多轮对话
- 平均每个对话4.5轮交互

**任务目标**：通过多轮对话与Oracle交互，准确到达目标位置

---

### TEACh (Task-driven Embodied Agents that Chat)

**基本信息：**

- **发布时间**：2021年
- **任务类型**：任务驱动的具身智能体通信

**特点：**

- 结合对话、视觉和动作执行
- 支持更复杂的任务指令和交互

---

### HA-VLN (Human-Aware VLN)

**基本信息：**

- **发布时间**：2025年
- **任务类型**：人群动态、多人体交互导航
- **环境**：室内/混合环境

**特点：**

- 支持离散与连续导航
- 考虑人群约束和多人体交互，是向真实场景迁移的重要基准

---

## 需求导向数据集

### DDN (Demand-driven Navigation)

**基本信息：**

- **任务类型**：基于抽象需求的导航

**特点：**

- 智能体需要将抽象需求映射到具体目标
- 更高层次的语义理解挑战

---

## 特殊场景数据集

### AerialVLN

**基本信息：**

- **任务类型**：空中无人机导航
- **场景类型**：室外、空中视角

**特点：**

- 专门针对无人机等飞行器的导航任务
- 俯视视角与室内导航有显著差异

---

### CityNav

**基本信息：**

- **发布时间**：2025年
- **任务类型**：空中视觉-语言导航
- **环境**：真实城市航拍

**特点：**

- 大规模实景城市航拍导航
- 结合地标与语义地图输入，适合无人机和户外导航研究

---

### OpenFly

**基本信息：**

- **发布时间**：2025年
- **任务类型**：空中导航
- **环境**：多场景航拍

**特点：**

- 包含10万条航行轨迹
- 大规模 aerial VLN 基准

---

## 数据集对比

| 数据集 | 发布年份 | 任务类型 | 环境类型 | 交互方式 | 主要特点 |
|--------|----------|----------|----------|----------|----------|
| R2R | 2018 | 指令导向 | 室内 | 单轮 | 经典基准，离散导航 |
| R4R | 2019 | 指令导向 | 室内 | 单轮 | 长路径，高难度 |
| RxR | 2020 | 指令导向 | 室内 | 单轮 | 多语言支持 |
| VLN-CE | 2020+ | 指令导向 | 室内连续 | 单轮 | 连续控制 |
| REVERIE | 2020 | 目标导向 | 室内 | 单轮 | 导航+物体定位 |
| SOON | 2020 | 目标导向 | 室内 | 单轮 | 语义理解 |
| LHPR-VLN | 2025 | 目标导向 | 室内连续 | 单轮 | 长程规划 |
| CVDN | 2019 | 对话导航 | 室内 | 多轮 | 对话交互 |
| TEACh | 2021 | 对话导航 | 室内 | 多轮 | 任务驱动 |
| HA-VLN | 2025 | 对话导航 | 室内/混合 | 多轮 | 人机交互 |
| DDN | 2021 | 需求导向 | 室内 | 单轮 | 抽象需求推理 |
| AerialVLN | 2021 | 指令导向 | 室外/空中 | 单轮 | 无人机导航 |
| CityNav | 2025 | 指令导向 | 实景城市 | 单轮 | 城市航拍导航 |
| OpenFly | 2025 | 指令导向 | 多场景航拍 | 单轮 | 大规模航拍 |


# VLN主流模拟器

VLN研究需要高质量的3D仿真环境来训练和测试导航模型。以下是VLN领域最常用的主流模拟器（含最新更新和趋势）：

## Matterport3D Simulator

**基本信息：**
- **开发者**：Peter Anderson et al.
- **发布时间**：2018年
- **开源地址**：[GitHub](https://github.com/peteanderson80/Matterport3DSimulator)

**核心特点：**
- **真实场景扫描**：基于Matterport3D数据集，包含90个真实室内环境的高精度3D扫描
- **全景视图**：提供360度全景RGB-D图像
- **离散导航**：采用预定义的导航图，智能体在固定视点间移动
- **高效渲染**：优化的渲染引擎，支持快速视觉观测生成
- **经典基准**：R2R、R4R等经典数据集的官方模拟器

**应用场景：**
- 指令导向的室内导航任务（R2R、R4R）
- 离散动作空间的VLN研究
- 基于真实场景的导航模型训练

**优势：**
- 真实感强，场景来自实际建筑扫描
- 与经典VLN数据集无缝集成
- 社区支持完善，大量研究基于此平台

**局限性：**
- 仅支持离散导航，灵活性受限
- 物理交互能力有限
- 场景数量相对较少（90个环境）

---

## Habitat

**基本信息：**
- **开发者**：Facebook AI Research (FAIR)
- **发布时间**：2019年（最新3.1版本2024–2025年更新）
- **开源地址**：[GitHub](https://github.com/facebookresearch/habitat-lab)

**核心特点：**
- **高性能仿真**：超快速渲染（10,000+ FPS）
- **连续环境**：支持连续动作空间和自由移动
- **多数据集支持**：兼容Matterport3D、Gibson、HM3D、LHPR-VLN等
- **模块化设计**：灵活的任务定义和传感器配置
- **Sim2Real支持**：提供真实机器人部署工具链
- **新特性**：
  - 动态环境支持（移动物体/人群）
  - 空中和户外环境支持
  - 长程任务和复杂子任务支持

**应用场景：**
- 连续动作空间导航研究（VLN-CE）
- 长视距任务（LHPR-VLN）
- 目标导航（ObjectNav）、语义导航（SemanticNav）
- 具身AI和Sim2Real研究

**优势：**
- 仿真速度极快，训练效率高
- 支持连续导航，更贴近真实机器人控制
- 大规模数据集（HM3D 800+场景）
- 动态场景、空中任务支持
- 强大的扩展性和社区生态

**局限性：**
- 配置复杂，学习曲线陡
- 对硬件要求较高（GPU加速）

---

## NVIDIA Isaac Sim / Isaac Lab

**基本信息：**
- **开发者**：NVIDIA
- **核心组件**：
  - **Isaac Sim**：基于 NVIDIA Omniverse 的高保真机器人仿真环境
  - **Isaac Lab**：基于 Isaac Sim 的模块化机器人学习与强化学习框架（GPU 加速）
- **开源地址**：
  - Isaac Lab 文档： https://isaac-sim.github.io/IsaacLab/main/index.html :contentReference[oaicite:0]{index=0}
  - Isaac Sim 官方页面： https://developer.nvidia.com/isaac-sim :contentReference[oaicite:1]{index=1}

**核心特点：**
- **高保真物理与渲染**：基于 RTX 加速的 PhysX 物理引擎与真实感渲染，可模拟碰撞、摩擦、传感器噪声等真实物理特性 :contentReference[oaicite:2]{index=2}
- **机器人学习集成**：Isaac Lab 提供强化学习、模仿学习、策略训练等端到端机器人学习工作流，可批量训练数千个并行环境 :contentReference[oaicite:3]{index=3}
- **多机平台资产库**：包括四旋翼、差分驱动机器人、步态机器人、机械臂等多种机器人模型，可自定义场景与任务 :contentReference[oaicite:4]{index=4}
- **导航与控制支持**：
  - 支持 ROS2、Nav2 等机器人导航栈集成，可用于路径规划与多机器人导航测试 :contentReference[oaicite:5]{index=5}
  - 虽主要用于强化学习与策略训练，但同样可用于评估视觉导航策略、连续控制与视觉感知组合任务
- **数据生成与 Sim‑to‑Real**：结合 Omniverse Replicator，可生成训练用合成数据并辅助现实迁移训练 :contentReference[oaicite:6]{index=6}

**应用场景：**
- 连续控制与导航策略训练（强化学习 / 模仿学习）
- 多传感器 SLAM、视觉感知与导航策略评估
- 多机器人协作与动态环境测试
- 合成数据生成与 Sim‑to‑Real 迁移训练

**优势：**
- **高保真模拟**：比传统离散图导航能更真实模拟连续物理行为与多传感器数据
- **学习框架支持**：内置强化学习训练工作流，可扩展到大规模并行环境
- **集成生态**：与 Omniverse、ROS、RTX GPU 加速等生态联动良好

**局限性：**
- **复杂度高**：上手门槛比简易模拟器如 Habitat、AI2‑THOR 更陡峭
- **计算资源要求高**：需要强 GPU 才能充分利用高保真渲染与物理仿真
- **目前在视觉导航（VLN）Benchmark 领域的专用数据集支持较少**：相比 Matterport/Habitat 等，社区内 VLN benchmark 评测还不如它们成熟

---

## AI2-THOR

**基本信息：**
- **开发者**：Allen Institute for AI
- **发布时间**：2017年（持续更新，最新4.0版本）
- **开源地址**：[官网](https://ai2thor.allenai.org/)

**核心特点：**
- **物理交互**：基于Unity3D，支持完整物理模拟
- **可交互对象**：环境中的物体可抓取、移动、操作
- **多样化场景**：厨房、卧室、客厅、浴室等200+场景
- **语义分割**：内置语义标注和实例分割
- **多智能体支持**：支持同时多个智能体任务
- **新特性**：
  - 多智能体协作
  - 可定制动作和交互
  - 可与VLN-CE、TEACh、EQA数据集结合

**应用场景：**
- 具身问答（EQA）
- 视觉语言导航+操作任务
- 家庭服务机器人研究

**优势：**
- 强大的物理引擎和真实物体交互
- 可多模态任务训练
- API友好，易上手

**局限性：**
- 渲染速度较慢
- 场景规模相对较小
- 资源消耗大

---

## Gibson / iGibson

**基本信息：**
- **开发者**：Stanford University
- **Gibson发布时间**：2018年
- **iGibson发布时间**：2021–2024（最新3.0版本）
- **开源地址**：[iGibson GitHub](https://github.com/StanfordVL/iGibson)

**核心特点：**

**Gibson 1.0**：
- 基于真实建筑扫描（1000+）
- 快速光栅化渲染
- 支持基础物理模拟

**iGibson 3.0**：
- **交互式场景**：完整物理交互和对象操作
- **逼真渲染**：PBR物理渲染
- **语义信息**：丰富的语义标注和物体属性
- **大规模场景**：完整房屋、办公楼等
- **任务多样性**：导航、操作、家务任务
- **新特性**：
  - 动态物体与人群模拟
  - 多智能体与社交导航约束
  - Sim2Real优化

**应用场景：**
- 大规模室内导航
- 导航+操作任务
- Sim2Real迁移研究
- 家庭服务机器人仿真

**优势：**
- 场景数量多，环境多样性高
- 真实感强，基于实际建筑扫描
- iGibson 3.0功能全面，支持复杂交互

**局限性：**
- 安装复杂
- 部分场景质量参差不齐

---

## AirSim

**基本信息：**
- **开发者**：Microsoft
- **发布时间**：2017年（持续更新）
- **开源地址**：[GitHub](https://github.com/microsoft/AirSim)

**核心特点：**
- **无人机/车辆仿真**：面向飞行器和地面车辆
- **高保真物理**：基于Unreal或Unity
- **多传感器支持**：相机、LiDAR、IMU、GPS
- **新特性**：
  - 城市大规模航拍场景
  - 长航程导航、多机协作
  - 与CityNav/OpenFly数据集配合

**应用场景：**
- 空中VLN（AerialVLN）
- 无人机导航与控制
- 自动驾驶与户外导航任务

**优势：**
- 专业飞行器仿真平台
- 高精度物理模拟
- 支持大规模户外环境

**局限性：**
- 室内导航支持有限
- 配置复杂，对硬件要求高

---

## 模拟器对比

| 模拟器 | 环境类型 | 动作空间 | 物理交互 | 渲染速度 | 主要应用 | 场景数量 | 新增特性 (2024–2025) |
|--------|----------|----------|----------|----------|----------|----------|---------------------|
| Matterport3D | 室内 | 离散 | 有限 | 快 | R2R/R4R | 90 | 保持经典基准 |
| Habitat 3.1 | 室内/空中/户外 | 连续 | 基础 | 极快 | VLN-CE, LHPR-VLN | 800+ (HM3D) | 动态物体、空中/长程导航、Sim2Real强化 |
| Isaac Sim / Lab | 室内/室外/空中 | 连续 | 强 | 高 | 强化学习、连续VLN | 可定制 | 高保真物理、动态环境、多机协作、Sim2Real |
| AI2-THOR 4.0 | 室内 | 离散/连续 | 强 | 中等 | 交互任务 | 200+ | 多智能体、可定制交互、家庭场景扩大 |
| iGibson 3.0 | 室内 | 连续 | 强 | 快 | 综合任务 | 1000+ | 动态人群、社交导航、Sim2Real强化 |
| AirSim | 室内外 | 连续 | 强 | 中等 | 无人机/车辆 | 可定制 | 城市航拍、大规模航程、多机协作 |
| iThorAir / Aerial Sim | 室外/空中 | 连续 | 基础 | 中等 | 空中VLN | 可定制 | 多机协作、长程规划、动态障碍物 |

---

## 选择建议

- **经典VLN基准（R2R/R4R）**：Matterport3D Simulator  
- **连续环境与长程任务**：Habitat 3.1  
- **需要物理交互任务**：AI2-THOR 4.0 / iGibson 3.0  
- **无人机/空中导航**：AirSim / Isaac Sim  
- **大规模场景训练**：Gibson/iGibson 或 Habitat + HM3D  
- **Sim-to-Real部署**：Habitat 3.1 / iGibson 3.0 / Isaac Sim


# 评估指标

评估指标是衡量VLN模型性能的关键工具。一个完善的评估体系应该同时考虑导航的**精度**、**效率**和**轨迹质量**。以下是VLN领域最常用的评估指标：

## 导航精度指标

### Success Rate (SR)

**定义：**
成功到达目标位置的episode比例。

**计算方法：**
$$
SR = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[d_i < \tau]
$$

其中：
- $N$ 是测试episode的总数
- $d_i$ 是第 $i$ 个episode结束时智能体与目标位置的距离
- $\tau$ 是成功阈值（通常设为3米）
- $\mathbb{1}[\cdot]$ 是指示函数，条件满足时为1，否则为0

**取值范围：**
- 0 到 1（或0%到100%）
- 越高越好

**优点：**
- 直观易懂，反映任务完成率
- 最常用的主要评估指标

**缺点：**
- 忽略了导航效率（路径长度）
- 对成功阈值敏感

### Navigation Error (NE)

**定义：**
智能体停止时与目标位置的平均距离误差（米）。

**计算方法：**
$$
NE = \frac{1}{N} \sum_{i=1}^{N} d_i
$$

其中：
- $d_i$ 是第 $i$ 个episode结束时智能体与目标的欧氏距离

**取值范围：**
- $[0, +\infty)$ 米
- 越低越好

**优点：**
- 提供连续的性能度量
- 不依赖成功阈值的设定

**缺点：**
- 受场景规模影响较大
- 难以跨数据集比较

### Oracle Success Rate (OSR)

**定义：**
在整个导航轨迹中，智能体曾经距离目标最近的位置是否满足成功条件。

**计算方法：**
$$
OSR = \frac{1}{N} \sum_{i=1}^{N} \mathbb{1}[\min_{t} d_i^{(t)} < \tau]
$$

其中：
- $d_i^{(t)}$ 是第 $i$ 个episode在时间步 $t$ 时智能体与目标的距离
- $\min_{t} d_i^{(t)}$ 是整个轨迹中与目标的最小距离

**取值范围：**
- 0 到 1（或0%到100%）
- 越高越好

**优点：**
- 评估智能体是否"到过"目标附近
- 反映路径规划的潜在能力
- 有助于区分"到达但没停"和"从未到达"两种失败情况

**缺点：**
- 不能反映最终导航结果
- 通常作为辅助指标使用

## 导航效率指标

### Success weighted by Path Length (SPL)

**定义：**
考虑路径效率的成功率，同时衡量成功率和路径长度。

**计算方法：**
$$
SPL = \frac{1}{N} \sum_{i=1}^{N} S_i \frac{l_i^*}{\max(p_i, l_i^*)}
$$

其中：
- $S_i$ 是成功指示符（到达目标为1，否则为0）
- $l_i^*$ 是最短路径长度（从起点到终点的理论最短距离）
- $p_i$ 是智能体实际走过的路径长度
- $\max(p_i, l_i^*)$ 确保分母不小于最短路径

**取值范围：**
- 0 到 1
- 越高越好
- SPL = 1 表示以最短路径成功到达目标

**优点：**
- **最重要的综合指标**，同时考虑成功率和效率
- 惩罚绕路行为，鼓励高效导航
- 被广泛用作主要性能指标（与SR并列）

**缺点：**
- 需要计算最短路径（需要环境图信息）
- 对失败的episode惩罚较重（直接计为0）

**典型取值：**
- 早期模型（2018-2019）：R2R上SPL约20-30%
- 中期模型（2020-2021）：R2R上SPL约30-40%
- 近期模型（2022-2024）：R2R上SPL约40-50%
- SOTA模型（2024+）：R2R上SPL可达50%以上

### Coverage weighted by Length Score (CLS)

**定义：**
衡量智能体按照指令覆盖参考路径的程度，同时考虑效率。

**计算方法：**
$$
CLS = \frac{1}{N} \sum_{i=1}^{N} \frac{C_i \times l_i^*}{\max(p_i, l_i^*)}
$$

其中：
- $C_i$ 是覆盖率（智能体经过的参考路径节点比例）
- $l_i^*$ 是参考路径长度
- $p_i$ 是实际路径长度

**取值范围：**
- 0 到 1
- 越高越好

**优点：**
- 评估轨迹与参考路径的匹配度
- 适合评估指令跟随能力

**缺点：**
- 依赖参考路径的质量
- 计算相对复杂

## 轨迹质量指标

### normalized Dynamic Time Warping (nDTW)

**定义：**
衡量智能体轨迹与参考路径之间的相似度，使用动态时间规整算法。

**计算方法：**
$$
nDTW = e^{-\frac{DTW(\mathcal{P}_{agent}, \mathcal{P}_{ref})}{\sigma}}
$$

其中：
- $DTW(\cdot, \cdot)$ 是动态时间规整距离
- $\mathcal{P}_{agent}$ 是智能体的实际轨迹
- $\mathcal{P}_{ref}$ 是参考路径
- $\sigma$ 是归一化参数

**取值范围：**
- 0 到 1
- 越高越好
- nDTW = 1 表示轨迹完全匹配

**优点：**
- 评估轨迹的时序一致性
- 对轨迹的局部偏差容忍度高
- 考虑了路径的整体形状

**缺点：**
- 计算复杂度较高
- 需要时序对齐，计算开销大

### Success weighted by normalized Dynamic Time Warping (SDTW)

**定义：**
结合成功率和轨迹相似度的综合指标。

**计算方法：**
$$
SDTW = \frac{1}{N} \sum_{i=1}^{N} S_i \cdot nDTW_i
$$

**优点：**
- 同时考虑成功率和轨迹质量
- 更全面的性能评估

## 其他辅助指标

### Trajectory Length (TL)

**定义：**
智能体实际走过的平均路径长度。

**用途：**
- 分析导航效率
- 检测模型是否过度探索或原地打转

### Steps Taken

**定义：**
智能体完成任务所需的平均步数。

**用途：**
- 评估导航速度
- 分析决策效率

### Collision Rate

**定义：**
发生碰撞的步数占总步数的比例。

**用途：**
- 评估导航安全性（在连续环境中）
- 检测路径规划质量

### Human Collision Rate

**定义：**
与动态行人发生碰撞的次数（用于社交导航）。

**用途：**
- 评估社交导航能力
- 测试动态避障性能

## 评估指标总结

### 按重要性分类

**核心指标（必须报告）：**
- Success Rate (SR)
- Success weighted by Path Length (SPL)

**辅助指标（建议报告）：**
- Navigation Error (NE)
- Oracle Success Rate (OSR)
- Trajectory Length (TL)

**高级指标（针对特定研究）：**
- nDTW / SDTW（轨迹质量研究）
- CLS（指令跟随研究）
- Collision Rate（安全性研究）

### 按任务类型选择

| 任务类型 | 主要指标 | 辅助指标 |
|----------|----------|----------|
| 指令导向（R2R） | SR, SPL | NE, OSR, nDTW |
| 目标导向（REVERIE） | SR, SPL | NE, 物体定位准确率 |
| 对话式导航（CVDN） | SR, SPL | 对话轮数, NE |
| 连续环境（VLN-CE） | SR, SPL | Collision Rate, TL |
| 社交导航 | SR, SPL | Human Collision Rate |

## 评估最佳实践

**1. 标准化报告：**
- 始终报告SR和SPL作为核心指标
- 分别报告seen（训练集环境）和unseen（测试集环境）性能
- 对于R2R，报告val_seen和val_unseen的结果

**2. 公平比较：**
- 使用相同的成功阈值（通常3米）
- 在相同的数据集划分上评估
- 明确说明是否使用ground truth路径

**3. 消融实验：**
- 分析各个模块对不同指标的影响
- 检查SR和SPL之间的权衡关系
- 评估模型在不同难度任务上的表现

**4. 可视化分析：**
- 绘制轨迹可视化图
- 分析失败案例
- 统计路径长度分布

## 指标演进趋势

VLN领域的评估体系经历了以下演进：

**早期（2018-2019）：**
- 主要使用SR和SPL
- 评估体系相对简单

**中期（2020-2021）：**
- 引入nDTW评估轨迹质量
- 增加CLS等细粒度指标

**近期（2022-至今）：**
- 从单一SR指标扩展至综合评估框架
- 引入安全性指标（碰撞率）
- 增加社交导航相关指标
- 强调Sim-to-Real场景下的实际部署性能

**未来方向：**
- 更注重真实世界部署的实用性指标
- 引入能耗、时间等物理约束
- 评估长期任务的鲁棒性和可靠性
- 多模态、多目标导航的综合评估


# 学习资源与框架

## 相关论文列表与库

- [VLN-Survey-with-Foundation-Models](https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models) - **最新！** 专门整理了结合大模型（LLM/VLM）的 VLN 论文。
- [Awesome-Embodied-AI](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln) - 涵盖了从导航（VLN）到操作（VLA）的全栈具身智能资源。
- [CVPR 2024/2025 Open Access](https://openaccess.thecvf.com/menu) - 搜索 "Vision-and-Language Navigation" 获取最新的顶会全文。

## 重要会议与研讨会

- **具身智能专项**：
  - [Embodied AI Workshop](https://embodied-ai.org/) (通常与 CVPR 联动)
  - CoRL (Conference on Robot Learning) - 目前 VLN 论文向机器人学迁移的主要阵地。
- **主流顶会**：CVPR, ICCV, ECCV, NeurIPS, ICRA, IROS。

## 开源项目与仿真平台

### 核心仿真器（Simulators）
- [Habitat-Sim / Habitat-Lab 3.0](https://github.com/facebookresearch/habitat-lab) - **行业标准**。支持多智能体协作、物理交互及 HM3DS 高清场景。
- [NVIDIA Isaac Lab (Omniverse)](https://github.com/isaac-sim/IsaacLab) - 基于高度物理拟真的导航环境，支持 Sim-to-Real 的跨越。
- [AI2-THOR](https://ai2thor.allenai.org/) - 侧重于室内物体的交互式导航（如打开冰箱、移动杯子）。

### 预训练框架与基座模型

- [VLN-BERT (Recurrent)](https://github.com/YicongHong/Recurrent-VLN-BERT) - **经典的判别式模型基准**。将 BERT 引入 VLN 领域，通过循环状态（Recurrent State）处理长序列导航指令，是很多后续工作的 Baseline。
- [DualVLN](https://github.com/panyuehu/DualVLN) - **全局与局部双系统导航框架（CVPR 2022）**。
  - **核心贡献**：提出了“双重规划”机制。**局部控制器**负责细粒度的跨模态对齐（行走），**全局规划器**基于维护的拓扑图进行大尺度路径优化。
  - **地位**：它标志着 VLN 模型从简单的“走一步看一步”进化到了具有“战略规划能力”的阶段。
- [DUET](https://github.com/cshizhe/DUET) - **大规模拓扑图表示学习的标杆**。通过交叉模态 Transformer 联合推理全局拓扑图和自然语言指令，是目前离散环境下最强的框架之一。
- [ETPNav](https://github.com/ChunHui-Zheng/ETPNav) - **演进式拓扑规划器 (2023-2024)**。在 DualVLN 和 DUET 的基础上，进一步解决了连续环境下动态构建拓扑图并进行高效路径规划的问题。
- [NaVid / Uni-NaVid](https://github.com/jzhzhang/NaVid-VLN-CE) - **2024-2025 视觉大模型（VLM）导航 SOTA**。
  - **特点**：首个不需要地图、不需要测距仪、仅依靠“纯视频流”输入即可完成连续环境导航的大模型，代表了目前从“基于几何”转向“基于端到端大模型语义”的研究前沿。

---

# 主流 VLN 研究框架 
目前的 VLN 研究已演进为以下四大主流框架：


## 1. 判别式跨模态匹配框架 
——Cross-modal Matching

- **起始时间**：2018–2020  
- **代表工作**：VLN-BERT, Recurrent VLN-BERT, PREVALENT  

### 核心思想  
该类方法将 VLN 建模为语言条件下的动作预测问题，通过文本编码器（BERT/LSTM）与视觉编码器（CNN/ViT）提取特征，并利用跨模态注意力机制实现指令语义与当前视觉观测的对齐，从而预测下一步动作。

本质上属于 **reactive policy learning**，不包含显式长期规划。

---

<div align="center">
  <img src="/images/VLN_Bert.png" width="100%" />
  <figcaption>Cross-modal Matching VLN 架构示意</figcaption>
</div>

---
### 典型 Pipeline  

```

Instruction → Text Encoder
Observation → Vision Encoder
↓
Cross-modal Attention Fusion
↓
Action Predictor (Policy Head)

```

---

### 优势  

- 结构简单，训练稳定。  
- 高效完成语言-视觉 grounding。  
- 奠定 VLN 的基础范式。

---

### 关键缺陷  

- ❌ 缺乏长期规划能力，仅建模 $$P(a_t \mid o_t, I)$$。  
- ❌ 无显式空间记忆，误差容易累积。  
- ❌ 泛化能力依赖训练分布，zero-shot 表现有限。  
- ❌ 难以处理复杂组合指令与回溯需求。

---


## 2. 语义地图与拓扑规划框架 
——Semantic Map & Graph-based Planning

- **起始时间**：2020–2022  
- **代表工作**：DUET, DualVLN, ETPNav, LagMemo  

### 核心思想  

该类方法引入显式环境建模机制，在导航过程中构建 **拓扑图或语义地图**，将空间连通关系与语义信息存储在外部记忆中。智能体基于该地图进行全局路径规划与局部执行。

以 LagMemo 为代表的方法进一步将语言中涉及的目标（如 sofa, kitchen）投影到所构建的语义地图上，实现 **language-grounded SLAM-style navigation**。

---

<div align="center">
  <img src="/images/lagmemo-language-injection.png" width="100%" />
  <figcaption>Semantic Map + Planning VLN 架构示意（LagMemo范式）</figcaption>
</div>

---
### 典型 Pipeline  

```

RGB / Depth → Mapping Module → Topological / Semantic Map
↑
Instruction → Language Parser → Goal / Constraint Projection
↓
Global Planner → Local Policy → Action

```

---

### 优势  

- 支持长距离规划与路径回溯。  
- 显式空间结构提升可解释性。  
- 有效缓解 partial observability。  

---

### 关键缺陷  

- ❌ 地图构建误差会累积传播。  
- ❌ 对动态环境与遮挡鲁棒性有限。  
- ❌ 语义投影通常依赖检测器或规则。  
- ❌ 缺乏高层语言推理能力。

---


## 3. 具身大模型与 VLA 统一框架 
——Foundation Models & VLA

- **起始时间**：2023–2024  
- **代表工作**：NaVid, LLM-Grounder, OpenVLA, InternVLN  

### 核心思想  

该类方法以大规模视觉语言模型（VLM / LLM）作为认知核心，将视觉观测、语言指令与动作统一建模为 token 序列，直接进行序列生成式决策。  

以 InternVLN 为代表的方法通常采用 **双系统结构**：  
- System-1：VLM 感知与语言理解。  
- System-2：基于推理的高层规划与子目标分解。  

从而实现从 matching 到 reasoning 的升级。

---

<div align="center">
  <img src="/images/dualvln-framework-overview.png" width="100%" />
  <figcaption>VLA / InternVLN 双系统架构示意</figcaption>
</div>

---
### 典型 Pipeline  

```

Image / History / Instruction → VLM Encoder
↓
Reasoning / Planner (LLM)
↓
Action Token Generator

```

---

### 优势  

- 强语言理解与常识推理能力。  
- 优秀 zero-shot 泛化。  
- 支持复杂指令分解与子目标规划。

---

### 关键缺陷  

- ❌ 推理成本高，实时性受限。  
- ❌ grounding 稳定性仍不足。  
- ❌ 易产生 hallucination 行为。  
- ❌ 缺乏低层物理执行建模。

---


<!-- ## 4. 连续环境与具身控制框架 (Continuous VLN & Control)

- **起始时间**：2021–2023  
- **代表工作**：VLN-CE, C-PST, Habitat-Web  

### 核心思想  

该类方法将 VLN 从离散动作空间扩展到真实物理连续空间，智能体需直接输出线速度与角速度，并处理滑移、碰撞与动态障碍问题。

VLN 不再仅是决策问题，而是融合 **planning + control + perception** 的系统工程问题。

---

### 典型 Pipeline  

```

Observation → State Estimation → Local Planner → Controller → Velocity Action
↑
Language-conditioned Goal

```

---

### 优势  

- 面向真实机器人部署。  
- 融合控制与语义决策。  
- 支持动态环境导航。

---

### 关键缺陷  

- ❌ sim2real gap 明显。  
- ❌ 控制误差放大高层决策错误。  
- ❌ 与语言推理结合仍较弱。  

--- -->

## 4. 生成式世界模型框架 
——Generative World Models

- **起始时间**：2024–2025  
- **代表工作**：Dynam3D (NeurIPS'25), V-A-World  

### 核心思想  

该类方法引入 **Predictive World Modeling**，使智能体在潜空间中预测不同动作可能带来的未来视觉结果，并在“想象空间”中搜索最优路径，从而减少真实环境中的试错。

VLN 从 reactive 转向 **deliberative planning with imagination**。

---
<div align="center">
  <img src="/images/world_model_architecture.png" width="100%" />
  <figcaption>Generative World Model VLN 架构示意</figcaption>
</div>

--
### 典型 Pipeline  

```

Current State → World Model → Future Rollouts
↓
Latent Space Planning
↓
Action

```

---

### 优势  

- 支持前瞻性规划。  
- 显著降低真实试错成本。  
- 更接近人类认知导航方式。

---

### 关键缺陷  

- ❌ 想象误差会累积。  
- ❌ 训练成本极高。  
- ❌ 与语言约束融合仍不成熟。

---



### 总结

VLN 的研究正在从感知对齐问题演进为融合 **语言理解、空间建模、规划推理、物理执行与想象预测** 的统一具身智能体。未来趋势是将 VLA 与 World Model 融合，形成“语言引导 + 想象规划 + 连续执行”的统一 VLN 系统架构。

---

### 2024-2026 技术趋势 (Key Trends)

| 技术趋势 | 描述 | 关键论文/项目 |
| :--- | :--- | :--- |
| **Long-Horizon** | 处理超长距离路径（>150步）及涉及多房间、跨楼层的复杂多阶段导航任务。 | LHPR-VLN (CVPR '25), L-VLN |
| **Dynamic Environment** | 针对真实世界中移动行人、开关门、光照突变等非稳态场景的适应性导航。 | DynamicVLN (2025), DynaNav |
| **World Models** | 引入预测学习，智能体通过生成未来视觉帧预测动作结果，实现“脑内模拟规划”。 | NVIDIA Cosmos, GenNav, NavMorph |
| **VLA Models** | **视觉-语言-动作 (VLA) 一体化**。直接从多模态输入输出底层控制指令，消除感知与动作的鸿沟。 | OpenVLA (2024), Helix, RT-2 |
| **Self-Evolving VLN** | **2025-2026 新趋势**。智能体在测试阶段通过自我反思和经验检索，在无需重新训练的情况下实现性能进化。 | **SE-VLN** (ICLR '26 / 2025), Reflection-Nav |
| **3DGS-based Map** | 利用 **3D高斯泼溅 (3D Gaussian Splatting)** 进行环境重建，提供比点云更精细、渲染更快的神经导航地图。 | GS-Nav (2025), Splat-Nav |

---

### 趋势深度解析

#### 1. 从“固定模型”到“自我演进 (Self-Evolving)”
这是目前最前沿的方向（如 **SE-VLN**）。传统的 VLN 模型在部署后能力是固定的，而最新的研究让智能体拥有一个“经验库”。当它导航失败时，它会记录失败原因并在下次遇到类似场景时进行**自我修正**。这种“闭环进化”让模型更像真实的生物。

#### 2. 生成式 AI 赋予智能体“想象力”
**World Models** 的引入改变了 VLN 的本质。以前智能体是“应激式”移动，现在它能利用 **Diffusion** 或 **Video Generation** 技术预判：“如果我右转，我会看到什么？”这种前瞻性规划显著降低了碰撞率。

#### 3. 具身大模型 (VLA) 的统领地位
随着 **OpenVLA** 等模型的成熟，VLN 不再是一个独立的视觉匹配任务，而是被纳入了通用的具身机器人大脑中。现在的趋势是：一个模型既能做 R2R 导航，也能在到达终点后完成“把杯子放进微波炉”的操作任务。


# VLN经典论文

## 1. DualVLN/InternVLN
——Ground Slow, Move Fast

**研究背景/问题**

VLN领域存在基本矛盾：强大的推理能力需要"慢思考"，而流畅的导航行动需要"快反应"。传统端到端模型存在三大瓶颈：动作碎片化（每一步都需调用大模型）、响应延迟高（无法实现高频控制）、缺乏层次协调（语义理解、全局规划和局部避障耦合在一起）。

**主要方法/创新点**

<div align="center">
  <img src="/images/dualvln-framework.png" width="100%" />
<figcaption>
DualVLN
</figcaption>
</div>

DualVLN提出双系统架构，将高级语义理解与低级轨迹执行解耦，形成互补的快慢系统：



**系统2（慢思考的"大脑"）：**
- **全局规划器**：基于Qwen-VL-2.5，以约2 Hz频率运行，负责理解指令、观察环境
- **像素级目标预测**：将3D导航任务转化为2D像素级目标定位问题（最远像素目标grounding）
- **自动生成训练数据**：通过三维到二维投影，将未来轨迹点投影到当前视角的2D图像上，利用深度信息过滤遮挡点，选择最远可见点作为"像素目标"
- **智能视角调整**：自主决定何时调整视角（如"左转/右转15°、抬头/低头15°"），最多支持4次连续视角调整，模仿人类寻路行为

<div align="center">
  <img src="/images/dualvln-framework-overview.png" width="100%" />
<figcaption>
DualVLN双系统框架架构
</figcaption>
</div>

**系统1（快行动的"小脑"）：**
- **高频轨迹生成**：轻量级扩散Transformer策略，以高达30 Hz频率运行
- **条件扩散模型**：融合低频语义条件（来自系统2）与高频视觉条件（实时RGB图像）
- **语义特征提取**：使用4个可学习的潜在查询向量从系统2的隐藏状态中提取任务相关语义特征
- **动态环境适配**：通过融合旧图像特征与最新图像特征，动态理解机器人位移和环境变化
- 输出平滑、连续、避障的轨迹（32个密集路径点）

<div align="center">
  <img src="/images/dualvln-system1-trajectory.png" width="100%" />
<figcaption>
系统1的高频轨迹生成
</figcaption>
</div>

**协同机制**：系统2每0.5秒规划一个新目标，系统1每0.03秒更新一次轨迹，实现"大脑想一步，小脑走十步"的高效控制。

**新基准Social-VLN**：
- 在VLN-CE环境中加入动态行走的人形机器人，沿任务路径放置，增加交互概率
- 引入Human Collision Rate指标，量化与行人的不安全交互次数

<div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/dualvln-social-vln-benchmark.png" width="100%" />
<figcaption>
Social-VLN基准测试场景示例
</figcaption>
</div>

**核心结果/发现**

- **仿真基准SOTA**：在VLN-CE和VLN-PE两大基准上均取得最佳成绩，尤其在R2R和RxR的未见场景中，成功率显著领先所有基线模型（NaVILA、StreamVLN等）
- **跨平台部署**：成功部署在轮式（Turtlebot4）、四足（Unitree Go2）、人形（Unitree G1）三种机器人平台上，仅搭载Intel RealSense D455单目RGB相机
- **多场景验证**：在办公室、食堂、街道、便利店等多种室内外场景中表现出色，能够规划平滑路径、避开动态行人、处理楼梯等复杂地形
- **零样本泛化**：展现出强大的零样本迁移能力，可直接迁移至长视程导航和户外自主探索任务
- **系统鲁棒性**：系统1对像素目标偏差具有鲁棒性，当系统2输出的像素目标存在方向正确但位置偏差时，系统1仍能通过实时RGB图像修正轨迹

**局限性**

系统在极端扰动下的鲁棒性仍需提升，仿真到现实的迁移效率有待优化。跨层表征对齐机制还需进一步改进，以实现更高效的双系统协同。

---

## 2. NavDP
——基于扩散模型的零样本导航规划

<div align="center">
  <img src="/images/navdp.png" width="100%" />
<figcaption>
NavDP双阶段推理框架
</figcaption>
</div>

**研究背景/问题**

机器人导航面临的核心挑战是如何在保证安全的前提下实现跨场景、跨平台的泛化能力。传统的导航方法往往依赖于显式地图构建或需要在目标环境中大量采集真实数据，这限制了其在实际场景中的部署效率。NavDP（Navigation with Diffusion Policy，上海AI Lab）提出通过大规模模拟数据训练，结合扩散模型生成候选轨迹和Critic网络评估安全性，实现零样本sim-to-real迁移。

**主要方法/创新点**

<div align="center">
  <img src="/images/navdp-framework.png" width="100%" />
<figcaption>
NavDP双阶段推理框架
</figcaption>
</div>

### 核心思路

扩散模型负责生成候选轨迹，Critic负责挑选安全路线

### 两阶段推理框架

- **第一阶段（策略生成）**：用RGB-D图像+导航目标，经策略Transformer编码后，通过扩散生成候选轨迹
- **第二阶段（安全评估）**：将生成轨迹与RGB-D token融合，再经共享Transformer与critic head，选择与目标无关的安全轨迹

### 模拟特权信息利用

- **生成器训练**：利用模拟环境中的全局最优规划器指导轨迹生成
- **Critic训练**：利用模拟环境的全局ESDF，从负样本轨迹中学习精细空间理解
- **数据增强**：对原始轨迹进行随机旋转和插值，生成混合轨迹增加多样性

### 多模态输入编码

- **输入**：单帧RGB-D图像+导航目标（四种类型：点目标、图像目标、轨迹目标、无目标）
- **深度处理**：裁剪至0.1-3.0 m，RGB经预训练DepthAnything ViT编码，深度由自训练ViT编码
- **Transformer解码器**：将512个RGB-D token压缩为16个融合token

### Real-to-Sim增强

- 采用Gaussian Splatting重建真实环境，提供高真实感的训练与评测平台
- 在训练集中加入27%的real-to-sim样本，可使目标场景成功率提升30%，且不损害泛化能力

<div align="center">
  <img src="/images/navdp-trajectory-visualization.png" width="100%" />
<figcaption>
NavDP模拟数据生成流程
</figcaption>
</div>

**核心结果/发现**

- **跨机器人平台泛化**：在不同机器人（Dingo、Go2、Galaxea R1）上稳定高于基线（GNM, ViNT, NoMad, iPlanner, ViPlanner, EgoPlanner）
- **零样本Sim-to-Real**：成功在Unitree Go2、Galaxea R1、Unitree G1上部署，室内外场景均表现良好，含动态行人干扰
- **数据规模与效率**：模拟数据生成速度约2,500条轨迹/GPU/天，比真实采集快20倍；数据集覆盖1244个场景、总长度363.2 km
- **模型组件贡献**：Critic模块是性能提升的关键，移除后性能显著下降；No-goal训练目标对整体避障行为影响最大
- **Real-to-Sim效果**：真实场景成功率提高30%，证明real-to-sim数据能显著提升sim-to-real成功率
- **高速避障**：>10Hz推理，支持2.0 m/s高速避障，动态场景下优于传统地图规划方法

**局限性**

NavDP的性能高度依赖于高质量的模拟数据训练，Real-to-Sim数据比例需要仔细平衡以避免过拟合特定场景。虽然在多种机器人平台上展现了良好的泛化能力，但在极端复杂环境（如密集人群、高度动态场景）下的鲁棒性仍有提升空间。此外，扩散模型的多步推理虽然提供了多样化的轨迹候选，但相比直接回归方法计算开销更大，对实时性要求极高的应用场景可能存在挑战。

---

## 3. NoMaD
——目标掩码扩散策略实现统一导航

**研究背景/问题**

传统机器人导航系统通常为探索（exploration）和目标导航（goal-conditioned navigation）分别训练独立的策略模型，这不仅增加了系统复杂度，也限制了跨任务的知识共享和泛化能力。NoMaD（Nomadic Multi-task Agent with Diffusion，伯克利，ICRA2024 Best Paper）提出通过统一的扩散策略框架，使用目标掩码机制同时建模任务特定行为（目标导向）和任务无关行为（探索），实现单一策略胜任多种导航任务。

**主要方法/创新点**

<div align="center">
  <img src="/images/nomad-framework.png" width="100%" />
<figcaption>
NoMaD目标掩码扩散策略框架
</figcaption>
</div>

### 核心思路

通过统一的扩散策略，同时建模任务特定和任务无关行为

### 两个关键组件

**目标掩码（Goal Masking）**
- 通过二值掩码控制策略是否关注目标图像，实现任务条件的灵活切换
- **训练时**：目标掩码以50%概率随机设置，使模型同时学习目标导向行为和探索行为
- **推理时**：根据任务需要设置掩码（探索时掩盖目标，导航时提供目标）

**扩散策略（Diffusion Policy）**
- 利用扩散模型生成多模态、无碰撞的动作序列
- 从随机噪声逐步迭代生成预测动作序列
- 动作分布既可在无目标条件下表达探索行为，也可在提供目标条件下收敛到目标导向行为

### 统一框架设计

- 通过Transformer编码视觉观测并结合扩散模型生成未来动作序列
- 同时支持任务特定行为（目标导向）和任务无关行为（探索）
- 使用大规模多样化数据集（GNM和SACSoN）进行端到端监督训练

<!-- <div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/nomad-goal-masking.png" width="100%" />
<figcaption>
NoMaD目标掩码机制示意图
</figcaption>
</div> -->

**核心结果/发现**

- **探索未知环境**：成功率达到98%，平均碰撞数仅0.2，超过最优基线Subgoal Diffusion约25%，且参数量仅为其1/15
- **目标导航**：在已知环境的目标导航任务中，成功率与最优基线相当，但计算资源需求更少
- **计算效率**：比现有方法计算效率提升约15倍，是首个成功在物理机器人上部署的目标条件动作扩散模型
- **统一策略优势**：联合训练能够学习共享表示和环境可操作性，单一策略即可胜任多种行为
- **编码器选择**：ViNT编码器配合注意力目标掩码效果最佳，成功率98%，碰撞数最少
- **多场景验证**：在6个复杂的室内外环境中表现优异

**局限性**

NoMaD的视觉编码器选择对性能影响较大，需要仔细调优以达到最佳效果。虽然ViT编码器具有更大的容量和表达能力，但其训练优化难度较高，收敛速度相对较慢。此外，目标掩码机制的随机采样比例（训练时50%）是一个关键超参数，在不同场景下可能需要针对性调整。尽管在多个室内外环境中表现优异，但在极端复杂、高度动态的场景（如密集人流、快速变化的障碍物）下的鲁棒性仍有进一步提升空间。

---
## 4. ODYSSEY
——Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks

**研究背景/问题**

在动态、非结构化环境中，机器人需要将移动性、操作和实时感知紧密结合才能执行复杂任务。现有研究大多局限于桌面场景，未能解决移动平台特有的感知受限和执行器范围有限的问题，且在开放世界环境中的泛化能力不足。

**主要方法/创新点**

ODYSSEY提出了一个统一的移动操作框架，包含分层规划和全身控制两大核心模块：

<div align="center">
  <img src="/images/odyssey-framework-overview.png" width="100%" />
<figcaption>
ODYSSEY框架整体架构
</figcaption>
</div>

**长期任务规划器：**
- **全局任务级规划**：融合RGB和LiDAR流构建场景的空-语义表示，利用预训练基础模型将实例图映射到场景中
- 使用GPT-4.1将自然语言指令分解为原子动作序列（导航、抓取、放置等），并输出粗略目标航路点
- 航路点投影到2D占用图，通过局部搜索确定无碰撞目标姿态

**局部操作：**
- 使用腕部安装的深度观测数据指导视觉-语言模型生成精确末端执行器姿态
- Qwen2.5-VL-72B-Instruct模型根据RGB观测和文本描述推断任务相关接触点
- 根据目标物体主轴和表面法线施加几何约束，确定末端执行器朝向

<div align="center">
  <img src="/images/odyssey-whole-body-control.png" width="100%" />
<figcaption>
两阶段全身控制策略训练流程
</figcaption>
</div>

**全身控制策略：**
- 单一网络将观测向量（运动指令、末端执行器目标、地面高度图、重力向量、本体感知状态等）映射到目标动作
- **两阶段训练**：第一阶段固定机械臂关节训练运动；第二阶段控制全部18个关节，采用地形不变末端执行器采样策略
- 引入步态奖励、频率奖励和末端执行器跟踪项，运用领域随机化增强适应性

**模拟基准测试：**
- 构建包含50个刚体物体、15个容器、30个关节结构、10个可拖动物体的多样化资产库
- 基准测试包括10个真实场景（室内家居、超市、餐厅、室外庭院等）
- 长期任务包含246个室内和58个室外变化，涉及抓取、重新定向、容器放置、关节操作等多种技能

<!-- <div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/odyssey-results-comparison.png" width="100%" />
<figcaption>
与基线方法的性能对比
</figcaption>
</div> -->

**核心结果/发现**

- **短期任务**：在ARNOLD基准测试上优于PerAct基线，仅依赖单个自我中心摄像头实现更强的泛化能力，在未见数据集上性能保持稳定
- **长期任务**：在8个长期移动操作任务上实现40%以上整体成功率，每个原子技能类别保持60%以上成功率，展现出可靠的协调能力
- **低层策略**：在基座速度跟踪方面优于RoboDuet基线，末端执行器姿态跟踪性能相当，且在不同地形上具有更强的适应性
- **Sim-to-Real迁移**：成功在Unitree Go2+Arx5平台上实现现实世界部署，在"导航到抓取"和"抓取和放置"任务中验证了框架的实用性

**局限性**

模型在物体几何形状的空间推理方面存在局限，导致夹爪对齐不佳和细长手柄或部分遮挡物品的定位不准确。此外，抓取小物体时偶尔失败，主要由于末端执行器跟踪和视觉感知精度不足。

---

## 5. PanoNav
——Mapless Zero-Shot Object Navigation

**研究背景/问题**

现有目标导航方法大多依赖深度传感器或预建地图来构建2.5D场景表示，限制了在真实环境中的适用性和泛化能力。零样本目标导航要求机器人识别和导航到超出预定义类别范围的对象，现有方法在开放词汇场景中表现有限。无地图方法通常只基于当前观测进行决策，忽略历史轨迹信息，容易陷入局部死锁。

**主要方法/创新点**

PanoNav是一个无地图、仅使用RGB图像的零样本目标导航框架，包含两个核心模块：

<div align="center">
  <img src="/images/panonav-framework-overview.png" width="100%" />
<figcaption>
PanoNav框架整体架构
</figcaption>
</div>

**全景场景解析（Panoramic Scene Parsing）：**

*局部方向解析：*
- **点阵图像增强**：将每个RGB图像转换为点阵图像，通过Scaffold方法增强平面位置理解，与RGB图像共同作为MLLM输入
- **空间关系图构建**：MLLM利用几何距离关系和平面位置关系，构建空间关系图，生成每个方向的详细描述（物体存在、空间关系、房间类型等）

<div align="center">
  <img src="/images/panonav-panoramic-parsing.png" width="100%" />
<figcaption>
全景场景解析模块：从RGB输入到局部方向描述
</figcaption>
</div>

*全局全景总结：*
- **环境整体感知**：对机器人周围环境进行整体分析，识别环境中存在的物体类型和当前房间类型（如厨房、走廊）
- **隐式自我定位**：通过全局总结提供隐式自我定位信息，帮助机器人理解其在更大环境中的位置

**动态记忆引导决策（Dynamic Memory-guided Decision-Making）：**

- **动态有界记忆队列**：存储最近的全局场景总结，队列长度固定，当队列满时新元素加入会移除最旧元素
- **决策过程**：
  - 记忆队列未满时：决策仅基于当前的局部描述和全局总结
  - 记忆队列满时：决策结合当前信息和历史记忆信息，避免重复探索已访问区域
- **动作选择**：决策结果包括导航方向和是否找到目标的标志，由运动控制器执行相应动作

<div align="center">
  <img src="/images/panonav-dynamic-memory.png" width="100%" />
<figcaption>
动态记忆引导决策机制
</figcaption>
</div>

**任务设置：**
- **观测数据**：每个时间步获取六个方向的RGB图像（间隔60度），形成全景视图，不依赖深度传感器或GPS
- **动作空间**：停止、前进（0.25米）、左转/右转（30度）、抬头/低头
- **任务目标**：在未见过的环境中根据语言指令找到目标对象，并导航至目标位置

**核心结果/发现**

- **性能优势**：在HM3D数据集上，PanoNav的成功率（SR）达到43.5%，SPL达到23.7%，显著优于PixNav（SR=37.9%，SPL=20.5%）和ZSON（SR=25.5%，SPL=12.6%），甚至超过部分依赖地图和闭词汇表的方法
- **死锁避免**：在高度欺骗性环境中，通过动态记忆机制实现48.0%成功率和19.2% SPL，逃离局部区域的逃逸率达82.0%
- **消融实验验证**：
  - 全景视图的重要性：仅使用三视图时性能显著下降（SR=19.5%，SPL=9.97%）
  - 解耦解析与决策的优势：解耦方法（SR=43.5%，SPL=23.7%）优于直接从MLLM输出决策（SR=38.5%，SPL=22.57%）
  - 动态记忆的关键作用：移除动态记忆后性能大幅下降（SR=38.5%，SPL=22.57%）

**局限性**

虽然PanoNav显著提升了无地图零样本导航性能，但未来仍需探索利用多模态信息（如语音、手势等）构建更强大的记忆队列，以进一步提高无地图目标导航的鲁棒性和泛化能力。

---

## 6. VLN-R1
——基于GRPO与Time-Decayed Reward的端到端导航

**研究背景/问题**

VLN是具身人工智能领域的一项核心挑战，要求智能体根据自然语言指令在真实世界环境中进行导航。传统的导航方法通常依赖离散的拓扑图和预定义的节点连接，限制了智能体在连续环境中的泛化能力。

**主要方法/创新点**

VLN-R1提出了一种创新的端到端框架，利用大型视觉-语言模型（LVLM）直接处理自我中心视频流，生成连续的导航动作。

<div align="center">
  <img src="/images/vln-r1-framework-overview.png" width="100%" />
<figcaption>
VLN-R1端到端框架整体架构
</figcaption>
</div>

**核心设计理念：**
- 构建能够实时处理自我中心视频流并生成连续导航动作的端到端框架
- 与传统方法依赖导航图或额外传感器不同，VLN-R1直接将视觉输入和自然语言指令转化为动作输出
- 提高系统通用性，增强在未见过环境中的适应能力

**主要组件：**

*VLN-Ego数据集：*
- **数据生成**：通过Habitat模拟器生成，包含自我中心视频流与未来动作预测的配对数据
- **三部分文本注释**：
  - 指令部分：自然语言导航指令（如"走到客厅的沙发旁"）
  - 视觉部分：包括历史帧和当前观察，提供自我中心的视觉信息
  - 动作部分：未来动作选择（前进、左转、右转、停止四种基本动作）
- **数据规模**：
  - 从Room-to-Room生成了60K个训练样本
  - 从Room-Across-Room生成了1.2M个训练样本
  - 覆盖了61个训练场景

<div align="center">
  <img src="/images/vln-ego-dataset.png" width="100%" />
<figcaption>
VLN-Ego数据集构建流程
</figcaption>
</div>

*长短期记忆采样：*
- 新颖的视频输入处理策略，用于动态平衡历史帧的重要性与当前观察的实时性
- 确保模型既能利用历史信息，又能快速响应当前环境变化
- 相比单一动作预测，多步动作预测结合历史上下文显著提升性能

**两阶段训练策略：**

<div align="center">
  <img src="/images/vln-r1-training-pipeline.png" width="100%" />
<figcaption>
VLN-R1两阶段训练流程
</figcaption>
</div>

*监督微调（SFT）阶段：*
- 模型的动作序列预测与专家演示对齐，通过监督学习优化输出文本
- 模型生成的多步动作序列文本与地面真值对齐，通过交叉熵损失进行优化
- 给定历史观察序列H_t、指令Z和当前观察O_t，模型预测n步未来动作序列

*强化微调（RFT）阶段：*
- 引入基于GRPO（Group Relative Policy Optimization）的强化学习方法
- 结合时间衰减奖励机制（TDR），进一步优化模型在长时程导航中的性能
- 超参数经过消融实验确定，生成次数选择8作为默认值

**时间衰减奖励机制（TDR）：**
- **核心思想**：通过引入衰减因子，平衡短期和长期奖励
- **作用机制**：使模型能够更关注近期的动作，同时考虑长期目标
- **优势**：用于评估多步动作预测的长期效果，优化长时程导航性能

<!-- <div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/vln-r1-tdr-mechanism.png" width="100%" />
<figcaption>
时间衰减奖励（TDR）机制示意图
</figcaption>
</div> -->

**模型架构：**
- **输入**：自我中心视频流和指令
- **输出**：多步未来动作序列
- **端到端设计**：消除了对导航图的依赖，使其在连续环境中表现出色

**核心结果/发现**

VLN-R1在VLN-CE（视觉-语言导航连续环境）基准上进行了全面测试：

**测试平台：**
- **Room-to-Room（R2R）**：要求智能体在单个房间内导航
- **Room-Across-Room（R4R）**：要求智能体跨房间导航，任务更具挑战性

**性能表现：**
- **R2R数据集**：展现了高效的导航能力和准确的任务完成率
- **R4R数据集**：通过强化微调显著提升了跨域适应性，小型2B模型的性能甚至接近7B模型
- **模型可扩展性**：证明了端到端框架在不同模型规模下的有效性

**消融实验验证：**
- **长短期记忆采样**：多步动作预测结合历史上下文显著提升性能，优于单一动作预测
- **TDR机制**：与传统奖励函数相比，TDR显著提高了长时程任务的成功率
- **生成次数**：从6增加到8时性能提升有限，因此选择8作为默认值

**技术优势：**
- 端到端设计实现了实时导航
- 结合LVLM的视觉-语言理解能力和强化学习的优化策略
- 展示了在任务特定推理中的潜力

**局限性**

论文内容相对简短，未详细说明具体的性能指标数值（如SR、SPL等）和与其他SOTA方法的详细对比。此外，对于Real-world部署的讨论较少，主要集中在仿真环境（Habitat）测试，缺乏真实机器人平台上的验证实验。

---

## 7. LagMemo 
——Language 3D Gaussian Splatting Memory for Multi-modal Open-vocabulary Multi-goal Visual Navigation

**研究背景/问题**

传统视觉导航方法受限于单目标、单模态和封闭类别设置，无法满足实际应用中多模态开放词汇表多目标导航的需求。现有方法如端到端强化学习依赖隐式状态编码导致泛化能力差，而模块化方法基于2D语义地图仅支持预定义类别，无法适应开放词汇场景。

**主要方法/创新点**

LagMemo提出了首个将语言特征融入3D Gaussian Splatting（3DGS）的视觉导航系统：

<div align="center">
  <img src="/images/lagmemo-framework-overview.png" width="100%" />
<figcaption>
LagMemo系统框架：先进行前沿探索构建语言3DGS记忆，再基于记忆进行多目标导航
</figcaption>
</div>

**语言3DGS记忆重建：**

*前沿探索与几何重建：*
- **前沿探索策略**：智能体首先进行基于前沿的环境探索，收集RGB-D图像和姿态信息
- **3DGS几何重建**：3D高斯由空间位置μ、颜色c、半径r和不透明度o参数化，通过颜色和深度渲染的几何损失优化
- **关键帧检索机制**：针对导航场景中帧间重叠有限导致的遗忘和表面空洞问题，引入帧池存储历史帧，周期性渲染并按PSNR评估，优先优化低保真帧

<div align="center">
  <img src="/images/lagmemo-language-injection.png" width="100%" />
<figcaption>
语言特征注入流程：SAM实例分割→CLIP特征提取→2D-3D特征关联→离散化码本
</figcaption>
</div>

*语言特征注入：*
- **实例级特征提取**：使用SAM生成实例掩码，通过特征splatting渲染逐像素语义特征，聚合掩码级特征并优化
- **两级码本量化**：粗分区联合考虑3D位置和语言特征，细分区仅基于语言特征细化类别
- **2D-3D特征关联**：对每个实例类别，渲染其高斯并评估与2D实例掩码的空间语义一致性，将高维CLIP实例特征分配给离散化的3D高斯语言类别
- **码本构建**：每个码本条目对应一簇3D高斯，富含CLIP特征，支持多模态查询（文本/图像）

**记忆引导的视觉导航：**

<div align="center">
  <img src="/images/lagmemo-navigation-pipeline.png" width="100%" />
<figcaption>
记忆引导导航管线：目标定位→路径点导航→目标验证→终点导航
</figcaption>
</div>

- **目标定位**：多模态输入目标（文本/图像）通过CLIP编码器编码，与码本计算余弦相似度定位候选实例，计算高斯质心并投影到2D障碍地图生成路径点
- **路径点导航**：使用Fast Marching Method（FMM）从当前位置规划无碰撞路径至路径点
- **目标验证与匹配**：到达路径点后全景扫描验证目标，对象/文本目标使用SEEM开放词汇实例分割和CLIP相似度二次验证，图像目标使用LightGlue特征匹配
- **终点导航**：确认目标可见后，利用SEEM掩码和深度信息将目标点投影到障碍地图，再次使用FMM导航至终点并执行STOP

**GOAT-Core基准数据集：**
- 针对GOAT-Bench质量问题策划高质量核心子集：每集20个子任务（原7.88个），平均13.37个独特类别（原4.82个），子任务间平均距离6.89m（原5.18m）
- 手动修正不准确文本描述，优先语义清晰的对象，限制所有子任务在单层楼
- 包含480个多模态子任务（163图像、158对象、159文本目标）

**核心结果/发现**

**目标定位任务：**
- 在GOAT-Core上总体成功率70.8%，显著优于VLMaps（58.8%）
- 分模态性能：对象88.4% vs 69.7%，图像56.4% vs 43.3%，文本66.8% vs 61.0%
- 语言3DGS保留详细3D空间上下文，实现精确定位，而VLMaps的2D网格压缩丢失关键几何信息

**多模态多目标视觉导航：**
- 平均成功率SR=56.3%，SPL=35.3%，在所有四个场景中均取得最高成功率
- 相比次优基线CoWs*，SR提升10%，相比Modular GOAT提升18%
- 文本导航任务优势尤为显著，充分展现丰富3D语义表示的优势
- 消融实验验证：移除关键帧机制导致PSNR从27.20降至21.15，定位准确率从70.8%降至66.3%；移除码本定位准确率骤降至34.6%
- 目标验证机制至关重要：无验证SR=41.3%，通用CLIP验证SR=46.7%，模态特定验证SR=56.3%

**局限性**

当前方法依赖静态固定容量记忆，在动态环境（重新排列、增删物体）中适应性不足。全场景3D表示内存密集，需要层次化或多分辨率高斯、不确定性感知剪枝和特征压缩。此外，目标定位能力依赖几何保真度，视角覆盖不足会留下盲区，未来需开发记忆感知的主动探索策略。

---



## 8. GaussNav 
——Gaussian Splatting for Visual Navigation

**研究背景/问题**

Instance ImageGoal Navigation (IIN)要求智能体在未探索环境中定位并导航至目标图像所描绘的特定对象实例，需要跨视角识别目标对象同时忽略干扰物。现有基于BEV地图的导航方法缺乏详细纹理表示，难以胜任实例级任务，无法保留场景的实例感知特征，不足以区分同类别的多个对象。

**主要方法/创新点**

GaussNav首次将3D Gaussian Splatting（3DGS）引入具身视觉导航，提出语义高斯地图表示：

<div align="center">
  <img src="/images/gaussnav-framework-overview.png" width="60%" />
<figcaption>
GaussNav整体框架：前沿探索→语义高斯构建→高斯导航
</figcaption>
</div>

**前沿探索（Frontier Exploration）：**
- 智能体同时维护探索地图和障碍地图，探索地图标记已探索区域，障碍地图标记场景中的障碍物
- 检测探索地图轮廓并排除障碍地图区域，将最近的前沿点设为路径点，迭代覆盖整个环境

**语义高斯构建（Semantic Gaussian Construction）：**

*几何重建：*
- **3DGS简化表示**：每个高斯由9个参数特征化：RGB颜色向量c、质心µ∈R³、半径r、不透明度o∈[0,1]、类别标签l
- **可微渲染**：通过alpha合成渲染RGB、深度和轮廓图像，支持新视角合成（NVS）
- **关键帧检索机制**：针对导航场景帧间重叠有限问题，存储历史帧并周期性渲染评估PSNR，优先优化低保真帧，采用两阶段优化（p1=30迭代新视点，p2=60迭代关键帧视点）

<div align="center">
  <img src="/images/gaussnav-semantic-gaussian-construction.png" width="60%" />
<figcaption>
语义高斯构建流程：高斯密集化与语义高斯更新交替进行
</figcaption>
</div>

*语义特征注入：*
- **实例分割**：使用Mask-RCNN为每个高斯分配语义标签
- **特征优化**：通过特征splatting渲染逐像素语义特征，优化特征损失以鼓励实例内一致性和实例间可分性
- **高斯聚类**：基于语义标签和3D位置聚类高斯，将场景中的对象分割为不同语义类别下的不同实例

**高斯导航（Gaussian Navigation）：**

<div align="center">
  <img src="/images/gaussnav-navigation-pipeline.png" width="80%" />
<figcaption>
高斯导航流程：分类器→渲染描述性图像→匹配与定位→路径规划
</figcaption>
</div>

- **分类器**：使用ResNet50对目标图像分类预测语义标签ˆlg，显著缩小搜索空间（如场景CrMo8WxCyVb从648个潜在观测减少到33个）
- **匹配与定位**：
  - 为每个候选实例通过NVS生成描述性图像（nv=1/3/5，θ=±15°/±30°水平和垂直旋转）
  - 使用DISK提取关键点和特征描述符，通过LightGlue匹配，选择匹配关键点数最多的候选对象
  - 使用DBSCAN聚类去除语义分割误差导致的离群点，精确定位目标实例
- **路径规划**：将语义高斯转换为点云并体素化投影到2D BEV网格，使用FMM生成最短距离场并规划路径

**创新要点：**
- 统一几何、语义和实例感知特征的地图表示，首次将3DGS应用于具身视觉导航
- 通过渲染描述性图像直接定位目标对象，无需额外探索或验证步骤
- 关键帧检索机制有效缓解导航场景中的遗忘和表面空洞问题

**核心结果/发现**

- **HM3D数据集性能**：SPL从0.347大幅提升至0.578（提升66.6%），成功率达72.5%，显著超越所有基线方法
- **效率优势**：运行帧率超过20 FPS，在模块化方法中效率最高，搜索空间优化显著（如CrMo8WxCyVb场景从648个观测点减少至33个）
- **消融实验验证**：
  - 移除分类器导致Success降至37.5%，SPL降至29.1%，但使用分类器后匹配时间减少2.5倍
  - 移除匹配模块Success降至44.4%，SPL降至35.3%
  - NVS对识别成功率有益，GT NVS可进一步提升性能（Success从72.3%升至74.7%）
  - 使用GT匹配模块Success提升至85.0%，GT目标定位Success达94.6%
- **渲染质量分析**：在HM3D验证集上PSNR最高可达40，深度渲染误差接近零，但部分高纹理场景重建质量欠佳
- **跨场景泛化**：在36个验证场景中表现稳定，语义高斯可视化展示了对多种场景复杂度和对象组成的鲁棒性

**局限性**

当前方法在高纹理环境中重建质量欠佳，导致NVS可能产生孔洞等伪影。错误源分析显示匹配失败和目标定位不准确仍有改进空间。语义高斯不适合直接路径规划，需转换为2D BEV网格，增加了计算开销。

---


## 9. VLFM
——Vision-Language Frontier Maps for Zero-Shot Semantic Navigation

**研究背景/问题**
零样本语义导航要求机器人在未见环境中高效定位目标对象，现有方法（如ESC、SemUtil）依赖物体检测器将视觉线索转化为文本后再用LLM/BERT进行语义推理，存在计算瓶颈且无法充分利用视觉-语言联合表征。如何直接从RGB观测中提取语义价值以指导前沿探索成为关键挑战。

**主要方法/创新点**

VLFM提出语言驱动的前沿价值图框架，实现端到端视觉-语义推理：

<div align="center">
  <img src="/images/vlfm-system-overview.png" width="100%" />
<figcaption>
VLFM系统架构：初始化、语义前沿探索、目标导航三阶段流程
</figcaption>
</div>

**核心机制：**

1. **前沿航点生成（Frontier Waypoint Generation）**
   - 利用深度和里程计构建2D占用地图，识别已探索与未探索区域边界作为前沿候选点
   - 每个前沿中点作为潜在导航航点

2. **价值图生成（Value Map Generation）**
   - 使用预训练BLIP-2视觉-语言模型直接从RGB图像计算语义价值分数
   - 文本提示："Seems like there is a <target object> ahead"
   - 输出余弦相似度分数并投影到俯视图价值图（双通道：语义分数+置信度分数）

<div align="center">
  <img src="/images/vlfm-value-map-generation.png" width="100%" />
<figcaption>
价值图生成流程：BLIP-2计算语义分数并投影到俯视图
</figcaption>
</div>

1. **置信度加权更新（Confidence-Weighted Averaging）**
   - 置信度分数基于像素相对光轴位置：$c_{i,j} = \cos^2(\theta/(\theta_{fov}/2) \times \pi/2)$
   - 重叠区域的语义值更新：$v_{i,j}^{new} = (c_{i,j}^{curr}v_{i,j}^{curr} + c_{i,j}^{prev}v_{i,j}^{prev})/(c_{i,j}^{curr} + c_{i,j}^{prev})$
   - 置信度更新偏向高置信值：$c_{i,j}^{new} = ((c_{i,j}^{curr})^2 + (c_{i,j}^{prev})^2)/(c_{i,j}^{curr} + c_{i,j}^{prev})$

<div align="center">
  <img src="/images/vlfm-confidence-weighting.png" width="100%" />
<figcaption>
置信度评分机制：光轴附近像素置信度最高，边缘递减
</figcaption>
</div>

1. **物体检测与导航**
   - YOLOv7用于COCO类别，Grounding-DINO用于开放词汇检测
   - Mobile-SAM提取目标轮廓，确定最近点作为目标航点
   - 使用VER训练的PointNav策略执行航点导航（纯几何理解，不依赖语义）

**关键创新：**
- 直接视觉-语义推理：绕过物体检测器，BLIP-2直接从RGB生成语义分数
- 空间化价值表征：将语义价值映射到俯视图网格，支持前沿选择
- 置信度驱动融合：动态平衡当前观测与历史信息

**核心结果/发现**
- **基准测试表现**：在Gibson、HM3D、MP3D三个数据集上均达到SOTA零样本性能
  - Gibson：SPL 52.2%、SR 84.0%（相比SemUtil提升+11.7% SPL、+14.7% SR）
  - HM3D：SPL 30.4%、SR 52.5%（相比ESC提升+8.1% SPL、+13.3% SR）
  - MP3D：SPL 17.5%、SR 36.4%（相比ESC提升+3.3% SPL、+7.7% SR）
- 超越部分有监督方法：在Gibson和MP3D数据集上优于SemExp、PONI等ObjectNav训练方法
- **消融实验**：置信度加权平均（Weighted avg.）在所有数据集上均优于简单替换（Replacement）和无权平均（Unweighted avg.）
- **真实世界部署**：成功在Boston Dynamics Spot机器人上部署，在办公楼环境中高效导航至未见目标对象，所有模型（BLIP-2、GroundingDINO、MobileSAM、ZoeDepth）实时运行于RTX 4090 MaxQ笔记本

**局限性**
仅支持单层楼导航（缺少z坐标里程计导致价值图重置困难），HM3D和MP3D中14.6%和9.6%的跨楼层任务失败；假定目标物体在默认相机高度可见，未来可探索主动相机控制、操作式搜索（如打开抽屉）及可复用的语义地图表征以支持长时程多任务规划。

## 10. Motus
——A Unified Latent Action World Model

**研究背景/问题**

当前具身智能体的理解、世界建模和控制能力被孤立地建模在不同模型中,这种碎片化阻碍了统一多模态生成能力的实现,也限制了从大规模异构数据中学习。现有方法将本应统一的系统分割为5个独立的建模任务:VLA(视觉-语言-动作模型)、WM(世界模型)、IDM(逆动力学模型)、VGM(视频生成模型)和视频-动作联合预测模型。两个核心挑战包括:如何在单一框架中统一这些多模态生成能力,以及如何利用大规模异构数据(互联网视频、自我中心人类演示、多机器人轨迹)进行动作专家的预训练。

**主要方法/创新点**

<div align="center">
  <img src="/images/motus-architecture-overview.png" width="90%" />
<figcaption>
Motus整体架构:Mixture-of-Transformer结构整合理解专家、视频生成专家和动作专家
</figcaption>
</div>

Motus提出了统一的潜在动作世界模型,通过以下创新实现五种建模范式的融合:

**1. Mixture-of-Transformer (MoT)架构:**
- **三模态联合注意力(Tri-model Joint Attention)**:将三个专家的多头自注意力层连接起来,在保留各专家特定功能的同时实现跨模态知识融合
  - 理解专家(Understanding Expert):基于Qwen3-VL-2B(253.5M参数),具备3D定位和空间理解能力
  - 视频生成专家(Video Generation Expert):采用Wan 2.2 5B作为视频基础模型
  - 动作专家(Action Expert):Transformer结构(641.5M参数),与Wan相同深度
- **总模型规模**:8B参数(VGM 5.00B + VLM 2.13B + Act Expert 641.5M + Und Expert 253.5M)

<!-- <div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/motus-tri-model-attention.png" width="100%" />
<figcaption>
三模态联合注意力机制详解
</figcaption>
</div> -->

**2. UniDiffuser式调度器:**
- 为视频和动作分配不同的时间步τ_o、τ_a和噪声尺度
- 支持五种推理模式的灵活切换:VLA、世界模型、IDM、VGM、视频-动作联合预测
- 使用rectified flow目标函数:
  - $$l_{\text{action}} = \mathbb{E} \left[ \left\| v^{\theta_a} - (\epsilon_a - a_{t+1:t+k}) \right\|^2 \right]$$
  - $$l_{\text{obs}} = \mathbb{E} \left[ \left\| v^{\theta_o} - (\epsilon_o - o_{t+1:t+k}) \right\|^2 \right]$$
  
<!-- - l_action = E[||v^θ_a - (ε_a - a_{t+1:t+k})||²]   - l_obs = E[||v^θ_o - (ε_o - o_{t+1:t+k})||²] -->

**3. 潜在动作(Latent Actions) - 像素级"增量动作":**

<div align="center">
  <img src="/images/motus-latent-action-vae.png" width="60%" />
<figcaption>
潜在动作VAE架构:从光流到潜在动作表示
</figcaption>
</div>

- **光流表示**:使用DPFlow计算光流作为通用运动表示,将其转换为RGB图像
- **深度压缩自编码器(DC-AE)**:将高维光流压缩为4×512维token,再通过轻量级编码器投影到14维潜在动作向量
- **训练策略**:混合90%无标注数据(自监督重建)+10%有标注轨迹(任务无关数据+标准演示)
- **分布对齐**:引入任务无关数据(AnyPos方法),使用Curobo随机采样目标机器人动作空间
- **损失函数**:$$\mathcal{L} = \mathcal{L}_{\text{recon}} + \lambda_a \left\| a_{\text{real}} - a_{\text{pred}} \right\|^2 + \beta \mathcal{L}_{\text{KL}}$$
<!-- - **损失函数**:L = L_recon + λ_a||a_real - a_pred||² + βL_KL -->

**4. 动作密集-视频稀疏预测策略:**
- 视频帧率:8帧 @ 5Hz
- 动作块:48步 @ 30Hz
- 通过下采样视频帧平衡token数量,防止过拟合视频预测而削弱动作预测能力

**5. 三阶段训练流程:**

<!-- <div align="center">
  <img src="/images/motus-training-pipeline.png" width="100%" />
<figcaption>
Motus三阶段训练流程与数据金字塔
</figcaption>
</div> -->

- **阶段1(视频生成)**:使用多机器人轨迹、自我中心人类视频和合成数据适配VGM(仅训练VGM,约8000 GPU小时)
- **阶段2(潜在动作统一训练)**:冻结VLM,在视频、语言和潜在动作上预训练整个Motus模型(约10000 GPU小时)
- **阶段3(监督微调)**:在目标机器人数据上使用真实动作微调(约400 GPU小时)

**6. 六层数据金字塔:**
- **Level 1**: Web数据(VGM和VLM预训练)
- **Level 2**: 自我中心人类视频(Egodex: 230,949样本)
- **Level 3**: 合成数据(RoboTwin: 27,500样本)
- **Level 4**: 任务无关数据(AnyPos: 1,000样本)
- **Level 5**: 多机器人任务轨迹数据(Agibot: 728,209 + RDT: 6,083 + RoboMind: 16,861)
- **Level 6**: 目标机器人任务轨迹数据(In-house: 2,000样本)

<div align="center">
  <img src="/images/motus-embodied-data-pyramid.png" width="100%" />
<figcaption>
具身数据金字塔:从Level 1到Level 6数据量递减但质量递增
</figcaption>
</div>

**核心结果/发现**

**仿真环境(RoboTwin 2.0)性能:**
- 随机化场景平均成功率:87.02%(Motus) vs 72.84%(X-VLA) vs 43.84%(π0.5)
- 相比X-VLA提升15%,相比π0.5提升45%
- 在50个任务上评估,包含强背景和环境随机化(随机背景、杂乱桌面、桌高扰动、随机光照)
- 清洁场景成功率:88.66%(Motus) vs 72.80%(X-VLA) vs 42.98%(π0.5)

<!-- <div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/motus-robotwin-results.png" width="100%" />
<figcaption>
RoboTwin 2.0仿真基准测试结果对比
</figcaption>
</div> -->

**真实世界实验:**
- **两个平台**:AC-One和Agilex-Aloha-2双臂机器人
- **9个复杂任务**:测试空间理解、可变形物体操作、精确流体控制、视觉理解、长时域规划
  - 任务包括:叠毛巾、使用滴滤咖啡机煮咖啡、研磨咖啡豆、将面包放入烤箱、从饮水机取水、倒水浇花、按键盘按键

- **AC-One平台**:平均部分成功率63.22%(Motus) vs 25.86%(无预训练) vs 14.79%(π0.5)
  - 突出任务:研磨咖啡豆92% vs 0%(无预训练),煮咖啡62% vs 0%,放立方体入盘100% vs 60%

- **Agilex-Aloha-2平台**:平均59.30%(Motus) vs 26.60%(无预训练) vs 48.60%(π0.5)
  - 突出任务:从饮水机取水96% vs 8%(无预训练),叠毛巾39% vs 0%

<!-- <div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/motus-real-world-tasks.png" width="100%" />
<figcaption>
Motus在真实世界复杂任务上的执行展示
</figcaption>
</div> -->

**其他基准测试:**
- **LIBERO-Long**:97.6%成功率(与X-VLA并列最优,达到state-of-the-art)
- **VLABench**: In Distribution平均0.48(vs π0.5的0.43),Cross Category平均0.25(vs π0.5的0.22)

**消融实验验证:**
- **训练阶段重要性**:完整Motus(阶段2预训练) 87.02% vs 仅阶段1 81.86%(+10.02%提升)
- **IDM模式性能**:动作MSE 0.014(Motus) vs 0.044(ResNet18+MLP) vs 0.122(DINOv2+MLP),显著优于专门训练的IDM基线
- **VLA模式竞争力**:83.90%成功率,与联合模式87.02%性能接近
- **世界模型生成质量**:FID 11.209,FVD 61.209,SSIM 0.866,PSNR 25.07(在两个平台上评估)

**五种统一模式实证验证:**
$$
\begin{aligned}
\text{1. VLA:} & \quad p(a_{t+1:t+k} \mid o_t, \ell) && \text{--- 从观察和语言预测动作} \\
\text{2. 世界模型:} & \quad p(o_{t+1:t+k} \mid o_t, a_{t+1:t+k}) && \text{--- 从当前观察和动作预测未来观察} \\
\text{3. IDM:} & \quad p(a_{t+1:t+k} \mid o_{t:t+k}) && \text{--- 从观察序列推断动作} \\
\text{4. VGM:} & \quad p(o_{t+1:t+k} \mid o_t, \ell) && \text{--- 从观察和语言生成未来视频} \\
\text{5. 联合预测:} & \quad p(o_{t+1:t+k}, a_{t+1:t+k} \mid o_t, \ell) && \text{--- 同时生成视频和动作}
\end{aligned}
$$

<!-- 1. VLA: p(a_{t+1:t+k} | o_t, ℓ) - 从观察和语言预测动作
2. 世界模型: p(o_{t+1:t+k} | o_t, a_{t+1:t+k}) - 从当前观察和动作预测未来观察
3. IDM: p(a_{t+1:t+k} | o_{t:t+k}) - 从观察序列推断动作
4. VGM: p(o_{t+1:t+k} | o_t, ℓ) - 从观察和语言生成未来视频
5. 视频-动作联合预测: p(o_{t+1:t+k}, a_{t+1:t+k} | o_t, ℓ) - 同时生成视频和动作 -->

<!-- <div align="center">
  <img src="https://r-c-group.github.io/blog_media/images/motus-unified-modes-visualization.png" width="100%" />
<figcaption>
Motus五种统一模式的可视化展示
</figcaption>
</div> -->

**局限性**

当前方法需要大量计算资源(总计约18,400 GPU小时训练)。某些复杂任务(如叠毛巾)的性能仍有限,部分成功率仅为39%。尽管通过潜在动作改进了跨具身泛化,但仍需进一步研究。未来工作将探索更先进的统一模型架构,追求更通用的运动先验,并从互联网规模的通用视频中学习潜在动作。此外,需要研究如何降低部署成本并提升模型在极端条件下的鲁棒性。

---
## 11. NavGPT 
——利用大语言模型进行视觉语言导航的显式推理
📄 **Paper**: https://arxiv.org/abs/2305.16986

**研究背景/问题**

现有的视觉语言导航(VLN)方法虽然性能较好,但其决策过程是隐式的、不可解释的。大语言模型(LLMs)在训练中展现出强大的推理能力和知识储备,但其在具身导航任务中的推理能力尚未被充分探索。研究的核心问题是:LLMs能否理解以文本形式描述的交互世界、动作及其后果,并利用这些信息解决导航任务?

**主要方法/创新点**

NavGPT是一个完全基于LLM的指令跟随导航系统,通过零样本方式执行视觉语言导航任务。系统包含四个核心组件:

<div align="center">
  <img src="/images/navgpt-architecture.png" width="100%" />
<figcaption>
NavGPT系统架构图
</figcaption>
</div>

1. **导航系统原则(Navigation System Principle)**: 定义VLN任务和LLM的基本推理格式与规则
2. **视觉基础模型(Visual Foundation Models)**: 使用BLIP-2将视觉观察转换为自然语言描述,使用Faster-RCNN检测物体并提取深度信息
3. **导航历史(Navigation History)**: 维护观察、推理和动作的三元组历史,使用GPT-3.5进行摘要以控制长度
4. **提示管理器(Prompt Manager)**: 将所有信息整合为LLM可理解的自然语言提示

关键创新在于**协同推理与动作(Synergizing Reasoning and Actions)**:
- 扩展动作空间为 A˜ = A ∪ R,其中R为推理轨迹
- 在每步导航前先生成推理(Thought),再做出动作决策(Action)
- 推理不触发环境交互,但能增强LLM的问题解决能力

<div align="center">
  <img src="/images/navgpt-visual-perception.png" width="100%" />
<figcaption>
视觉感知转换为语言描述的过程
</figcaption>
</div>

NavGPT展现出多种高级导航规划能力:
- 将指令分解为子目标
- 整合与导航相关的常识知识
- 从观察场景中识别地标
- 跟踪导航进度
- 处理异常情况并调整计划

**核心结果/发现**

在R2R val unseen数据集上的实验结果(使用GPT-4):
- Success Rate (SR): 34%
- SPL: 29%
- Oracle Success Rate (OSR): 42%

虽然与训练模型仍有约40%的性能差距,但NavGPT展示了LLM的强大能力:
- **高级规划能力**: 能够分解指令、识别地标、跟踪进度、适应异常
- **生成能力**: 可根据导航历史生成高质量的导航指令
- **空间意识**: 能够绘制准确的俯视图轨迹

人类评估显示LLM生成的推理质量可接受但仍有提升空间(准确性1.66/3.0,信息量1.93/3.0,合理性1.78/3.0)。

**局限性**

主要瓶颈在于:(1)视觉信号转换为自然语言时的信息损失;(2)历史观察摘要时的信息损失;(3)零样本性能与训练模型相比仍有较大差距(SR差距约40%)。未来方向建议采用多模态输入的LLMs或利用LLMs的高级推理能力来增强基于学习的模型。

---

## 12. NavGPT-2 
——释放大型视觉语言模型的导航推理能力
📄 **Paper**: https://arxiv.org/abs/2407.12366

**研究背景/问题**

虽然有将LLMs集成到VLN任务的努力,但存在两个极端方法的局限:(1)零样本方法依赖复杂的提示工程,存在信息损失且性能差距大(约40% SR);(2)微调方法虽然使用大规模LLMs,但性能仍落后于VLN专用模型,且丧失了LLMs的语言能力和可解释性。研究目标是在保持LLMs解释能力的同时,消除LLM-based agents与SOTA VLN专用模型之间的性能差距。

**主要方法/创新点**

NavGPT-2采用**冻结LLM + 导航策略网络**的混合架构,分两阶段训练:

<div align="center">
  <img src="/images/navgpt2-architecture.png" width="100%" />
<figcaption>
NavGPT-2模型架构
</figcaption>
</div>

**阶段一:视觉指令调优(Visual Instruction Tuning)**
- 基于InstructBLIP架构,使用Q-former将多视图图像编码为固定长度的视觉tokens
- 使用GPT-4V自动生成10K导航推理数据
- 仅微调Q-former和投影层,保持LLM和视觉编码器(EVA-CLIP ViT-g/14)冻结

<div align="center">
  <img src="/images/navgpt2-data-generation.png" width="100%" />
<figcaption>
GPT-4V导航推理数据生成流程
</figcaption>
</div>

**阶段二:图基础导航策略学习**
- 提取LLM隐藏层表示作为视觉-语言表征
- 采用拓扑图导航策略网络(源自DUET),包含:
  - 节点嵌入(Node Embedding):整合视觉特征、方向嵌入、步数嵌入
  - 跨模态编码(Cross-Modal Encoding):图感知自注意力(GASA)机制
  - 全局动作预测(Global Action Prediction):从整个构建的图中选择下一步
- 使用DAgger损失训练,保持VLM冻结

关键创新点:
1. **VLM隐表示作为视觉-语言表征**:将视觉特征投影到LLM的语言空间,实现更强的跨环境对齐
2. **数据高效**:利用LLM预训练权重,在50%数据量下即可达到DUET全量数据的性能
3. **保留语言能力**:冻结LLM使其保持生成导航推理和与人类交互的能力

**核心结果/发现**

在R2R数据集上的性能(NavGPT-2FlanT5-XXL, 5B参数):

| Split | SR | SPL | NE | OSR |
|-------|----|----|-----|-----|
| Val Unseen | 71% | 60% | 3.18 | 80% |
| Test Unseen | 72% | 60% | 3.33 | 80% |

主要发现:
- **消除性能差距**:在相同训练规模下,超越所有LLM-based方法,与DUET(SOTA VLN专用模型)性能相当
- **数据效率**:使用50% R2R数据即可达到DUET使用全量数据的性能
- **泛化能力**:
  - RxR数据集(细粒度指令):SR提升3.67%
  - HM3D数据集(未见环境):SR提升21.6%(47.2% vs 25.6%)
- **可解释性**:能够生成描述周围环境、识别导航进度、规划下一步的自然语言推理

<div align="center">
  <img src="/images/navgpt2-reasoning-examples.png" width="100%" />
<figcaption>
NavGPT-2生成的导航推理示例
</figcaption>
</div>

消融实验表明:
- 移除导航策略网络后性能大幅下降(SR从68%降至21%)
- FlanT5系列模型优于Vicuna系列(编码器-解码器架构优于纯解码器架构)
- 更强的视觉编码器对性能提升有限,主要增益来自LLM隐表示

**局限性**

(1)导航推理基于局部观察,未在VLM中建模历史,一致性有待提高;(2)推理与动作预测未严格同步;(3)存在幻觉问题(识别不存在的物体或误判方向);(4)交互能力未经充分评估。未来工作应聚焦于推理-动作同步机制、历史建模以及交互导航能力的开发。

### 系列对比总结

| 维度 | NavGPT (AAAI-2024) | NavGPT-2 (ECCV-2024) |
|------|-------------------|---------------------|
| **核心思路** | 纯LLM零样本导航 | 冻结LLM + 微调导航策略 |
| **训练方式** | 无需训练(零样本) | 两阶段训练(VLM微调+策略学习) |
| **性能(R2R SR)** | 34% | 72% (test unseen) |
| **推理能力** | 显式,基于提示工程 | 显式,基于指令调优 |
| **主要贡献** | 揭示LLM导航推理能力 | 消除LLM-agent与SOTA的性能差距 |
| **局限** | 性能差距大,信息损失严重 | 推理-动作同步不足,存在幻觉 |

两篇工作共同展示了LLMs在具身导航中的巨大潜力,从探索性的零样本方法发展到实用的混合架构,为构建可解释、可交互的通用导航智能体指明了方向。


---

## 13. GaussianAD (24)
——Gaussian-Centric End-to-End Autonomous Driving
📄 **Paper**: https://arxiv.org/abs/2412.10371   🛖**代码仓库**：https://github.com/wzzheng/GaussianAD

**研究背景/问题**

现有的端到端自动驾驶方法在场景表示上存在困境：密集表示（如鸟瞰图BEV、体素）虽然全面但计算开销大，影响决策推理的资源分配；稀疏表示（如实例框、地图元素）虽然高效但无法捕获细粒度的3D结构信息，可能遗漏关键信息（如不规则障碍物、交通信号灯、人体姿态等），不符合端到端驾驶"全面信息流"的理念。

**主要方法/创新点**

<div align="center">
  <img src="/images/gaussianad-framework-overview.png" width="100%" />
<figcaption>
GaussianAD整体框架：使用统一3D高斯初始化场景，通过4D稀疏卷积和可变形交叉注意力融合多帧环视图像特征，支持密集任务（3D占用）和稀疏任务（3D检测、地图构建、运动预测），并通过高斯流预测进行轨迹规划
</figcaption>
</div>

本文提出GaussianAD框架，核心创新包括：

1. **3D语义高斯表示**：使用稀疏的3D语义高斯（每个高斯由均值、协方差和语义logits表征）作为场景中间表示，既保持稀疏性又具备高斯混合的通用逼近能力和显式3D结构。

2. **高斯场景构建**：
   - 用统一分布的3D高斯初始化场景（默认25600个）
   - 采用4D稀疏卷积实现高斯间的时序交互
   - 通过可变形交叉注意力逐步融合多帧环视图像特征来细化高斯

3. **灵活的感知架构**：
   - 密集任务（3D占用预测）：使用高斯到体素映射（Gaussian-to-Voxel Splatting）
   - 稀疏任务（3D检测、地图构建）：使用3D稀疏卷积+稀疏最大池化，直接从高斯预测
   - 运动预测：通过agent tokens和map tokens的交叉注意力获得motion tokens

<div align="center">
  <img src="/images/gaussianad-training-pipeline.png" width="100%" />
<figcaption>
训练流程：支持不同标注组合的灵活训练，利用显式高斯表示进行仿射变换预测未来场景，用未来感知标签或未来场景表示作为监督
</figcaption>
</div>

4. **高斯流（Gaussian Flow）预测**：
   - 为每个高斯预测3D流（future displacement）来建模场景演化
   - 覆盖动态和静态元素的全面场景预测
   - 利用规划轨迹进行全局仿射变换，预测自车观察到的未来场景表示
   - 用未来场景表示差异作为预测和规划的显式监督

5. **端到端训练**：
   - 总损失：J_GaussianAD = J_perc + J_pred + J_plan
   - 支持不同标注组合：3D占用、3D检测、地图、运动、场景预测
   - 场景预测监督无需额外标注，只需未来感知标签

<div align="center">
  <img src="/images/gaussianad-visualization.png" width="100%" />
<figcaption>
可视化结果：包含3D占用预测、3D目标检测、语义地图构建和轨迹规划的综合结果
</figcaption>
</div>

**核心结果/发现**

在nuScenes数据集上的实验结果：

1. **端到端规划性能（SOTA）**：
   - L2位移误差：1s/2s/3s分别为0.40/0.64/0.88米，平均0.64米
   - 碰撞率：1s/2s/3s分别为0.09%/0.38%/0.81%，平均0.42%
   - 相比使用相同监督信号的OccNet，大幅优于其2.14米的平均L2误差

2. **3D感知能力**：
   - 3D占用预测：mIoU 22.12%，IoU 33.81%（优于GaussianFormer等专用方法）
   - 3D目标检测：mAP 0.19（略低于专注检测的方法，但作为多任务模型表现合理）

3. **4D占用预测**：
   - 未来1s/2s/3s的mIoU分别为6.29%/5.36%/4.58%
   - 验证了高斯流的场景预测能力，尽管GaussianAD是端到端多任务模型

4. **消融研究**：
   - 使用运动监督对降低碰撞率特别有效
   - 基于流的场景预测监督可达到类似效果，且无需额外标注
   - 高斯剪枝（-40%）轻微降低感知性能但改善规划性能

**局限性**

GaussianAD无法预测完全准确的场景演化，因为它没有考虑自车向前移动时新观察到的区域的补全。这导致在4D占用预测任务上的性能相比专用方法有所不足。

---


## 14. FSR-VLN (2025)
——基于层次化多模态场景图的快慢推理视觉语言导航

📄 **Paper**: https://arxiv.org/abs/2509.13733v3

**研究背景/问题**

视觉语言导航（VLN）是具身智能中的基础任务，但现有方法在长距离空间推理方面存在严重局限，特别是在长距离导航任务中表现出较低的成功率和较高的推理延迟。关键瓶颈在于缺乏持久的长距离空间记忆来编码、组织和检索环境知识。现有几何语义地图和3D场景图依赖预提取的视觉特征，缺乏与VLM的直接交互；而基于图像的拓扑方法虽然成功率高，但由于依赖视频字幕处理长序列而效率低下。

**主要方法/创新点**

<div align="center">
  <img src="/images/fsr-vln-system-overview.png" width="100%" />
<figcaption>
系统总览：HMSG构建与基于FSR的导航推理流程
</figcaption>
</div>

FSR-VLN提出了结合两大核心创新的新型导航系统：

**1. 层次化多模态场景图（HMSG）表示**

HMSG将环境组织为四个层级：
- **楼层节点（Floor nodes）**：存储楼层标识符、名称、最小/最大高度、PLY点云以及所含房间节点的引用
- **房间节点（Room nodes）**：包含ID、2D多边形边界、点云、语义属性（名称、CLIP嵌入）以及关联的视图和对象节点链接
- **视图节点（View nodes）**（新颖贡献）：表示房间内的特定视觉视角，存储CLIP嵌入、VLM生成的描述、相机位姿以及与对象的可见性关系。该层支持使用VLM对图像视图进行推理，同时增强对象级定位能力
- **对象节点（Object nodes）**：表示离散实例，具有几何属性（3D边界框、点云）、语义嵌入以及与父房间和可见视图的链接

每个节点编码多模态特征，包括几何属性、语义信息和拓扑连接。HMSG使用FAST-LIVO2 SLAM系统提取RGBD数据和位姿来构建，然后进行开放词汇实例映射。GPT-4o从图像视图推断房间名称，系统计算每个对象的平均深度以选择最佳代表视图。

<div align="center">
  <img src="/images/hmsg-representation.png" width="100%" />
<figcaption>
四层HMSG层次结构，每个节点包含多模态特征
</figcaption>
</div>

**2. 快慢导航推理（FSR）**

受人类认知双过程理论启发，FSR分三个阶段运行：

**阶段1：基于LLM的用户指令理解**
- 对于空间指令（如"办公室里的蓝色圆柱形凳子"）：LLM充当层次化概念解析器，将输入分解为楼层、区域和对象组件
- 对于非空间指令（如"我累了"）：LLM充当目标推理代理，根据用户意图识别最相关的对象或区域

**阶段2：快速匹配（直觉检索）**
- 在查询文本与HMSG视图层嵌入之间进行基于CLIP的相似度匹配以识别目标视图
- 并行进行对象级匹配，使用查询文本与对象嵌入之间的CLIP特征
- 通过层次化特征匹配高效检索候选房间、视图和对象

**阶段3：慢速推理（深思熟虑的精化）**
- VLM（GPT-4o）验证匹配的对象是否出现在其最佳视图中
- 如果验证失败，系统会：
  - 使用LLM对未匹配视图的文本描述进行推理
  - 比较快速匹配视图与LLM选择的视图
  - 应用VLM推理确定最终最优目标图像
  - 通过重新计算与最终视图中对象的CLIP相似度来更新目标对象

<div align="center">
  <img src="/images/fsr-navigation-reasoning.png" width="100%" />
<figcaption>
三阶段FSR流程：LLM指令理解、基于CLIP的快速匹配、基于VLM的慢速推理
</figcaption>
</div>

这种多阶段架构无缝集成了高效的特征空间匹配与鲁棒的VLM驱动视觉验证。慢速推理仅在快速直觉失败时激活，大幅减少推理时间的同时提高准确性。

**核心结果/发现**

FSR-VLN在长距离真实室内环境中对87条机器人采集的指令进行评估，涵盖四个不同类别（无需推理、需要推理、小物体、空间目标）：

- **成功率（SR）**：92%（80/87），显著优于基线方法：
  - 比MobilityVLA（34.5%）高167%
  - 比OK-Robot（60.9%）高51%
  - 比HOVSG（51.7%）高77%

- **检索成功率（RSR@Top1）**：在4-5米距离阈值下达到96.6%，在所有距离阈值下始终保持最佳性能

- **响应时间**：使用慢速推理平均5.5秒，仅使用快速匹配平均1.5秒
  - 与MobilityVLM（30秒）相比响应时间减少82%
  - 通过仅在快速匹配失败时激活慢速推理，实现高效实时性能

- **HM3D-SEM数据集**：RSR@Top1在1米处达到87%，显著优于HOVSG（52%）和osmAG-LLM（28%）

- **消融实验**：添加空间目标（ST）指令使RSR从72.4%提升到81.6%，结合导航推理（NR）进一步提升到92%，验证了两个组件的有效性

该系统已成功集成到Unitree-G1人形机器人的语音交互、规划和控制模块中，展示了具备自然语言交互能力的真实世界部署能力。

**局限性**

HMSG构建耗时，不适合实时建图。系统假设静态环境，限制了在动态场景中的适用性。未来工作将重点提高场景图构建效率、扩展对动态环境的鲁棒性，以及集成探索性导航能力以处理新颖或模糊的场景。


---

## 15. VLingNav: Embodied Navigation with Adaptive Reasoning and Visual-Assisted Linguistic Memory (2026)
——A Unified VLA Model with Adaptive CoT and Visual-Assisted Linguistic Memory
📄 **Paper**: https://wsakobe.github.io/VLingNav-web/

**精华**
该论文提出了VLingNav框架，通过自适应链式思考（AdaCoT）和视觉辅助语言记忆（VLingMem）赋予具身智能体认知能力，实现了高效且可解释的具身导航。其核心亮点在于动态推理机制和跨模态记忆，使其在各种具身导航基准测试中达到SOTA性能，并展示了强大的零样本迁移能力和跨任务泛化能力，为资源受限机器人平台上的智能导航提供了启发。

**研究背景/问题**
当前的具身导航VLA模型在复杂、长周期任务中缺乏明确的推理能力和持久性记忆，难以泛化到不同环境和任务变体。现有模型多为被动式系统，缺少自适应推理机制，并且依赖有限的上下文窗口，导致在复杂场景下无法有效规划和避免重复探索。

**主要方法/创新点**
本文提出了VLingNav，一个以语言驱动的VLA框架，旨在通过两个核心组件赋予具身智能体认知能力：

1.  **自适应链式思考 (Adaptive Chain-of-Thought, AdaCoT)**：受人类双进程理论启发，AdaCoT机制在必要时动态触发显式推理，使智能体能够根据任务复杂性在快速、直观执行和缓慢、深思熟虑的规划之间灵活切换。这解决了现有CoT方法中推理频率固定导致效率低下的问题。

<div align="center">
  <img src="/images/VLingNav_architecture.png" width="100%" />
<figcaption>
VLingNav 整体架构概述，展示了AdaCoT推理和VLingMem记忆模块。
</figcaption>
</div>

2.  **视觉辅助语言记忆 (Visual-Assisted Linguistic Memory, VLingMem)**：为了处理长周期的空间依赖性，VLingMem构建了一个持久的、跨模态的语义记忆，使智能体能够回忆过去的观察结果，防止重复探索，并推断动态环境中的移动趋势，从而确保在长时间交互中的连贯决策。

<div align="center">
  <img src="/images/VLingNav-CoT-labeling-pipeline.png" width="100%" />
<figcaption>
VLingNav的自适应CoT标注流程图。
</figcaption>
</div>

**训练数据和策略**：
- **Nav-AdaCoT-2.9M数据集**：构建了目前最大的具身导航数据集，包含推理标注和自适应CoT标注。
- **在线专家引导强化学习 (Online Expert-guided RL)**：在模仿学习（SFT）之后引入了在线专家引导RL阶段，使模型能够获得更鲁棒、自探索的导航行为，超越监督演示的局限性。

<div align="center">
  <img src="/images/VLingNav-online-training.png" width="100%" />
<figcaption>
在线后训练的混合rollout过程。
</figcaption>
</div>

**核心结果/发现**
- VLingNav在多项具身导航基准测试（如ObjectNav, EVT, ImageNav）上实现了最先进的性能。
- 在HM3Dv1 ObjectNav上，SR和SPL显著优于Uni-NaVid，展现了强大的探索和记忆能力。
- 在HM3D OVON上，VLingNav在所有测试拆分中均表现最佳，证明了其强大的跨领域泛化能力。
- 在EVT-Bench上，VLingNav在单目标跟踪和分心跟踪任务中均达到SOTA性能，尤其在复杂混乱场景中优势明显。
- 在Image Goal Navigation上，VLingNav的成功率和导航效率显著高于UniGoal，表明其先进的推理和规划能力。
- 在真实世界机器人平台上实现了零样本迁移，成功执行了未见过的导航任务，展示了强大的真实世界泛化和实用性。

**局限性**
- 当前模型主要依赖单目自我中心观测，这限制了其感知能力。未来工作可以探索多视角观测以提高导航效率。
- 模型采用单系统架构，限制了预测频率，可能影响在高度动态环境中的快速决策和障碍物处理。未来可升级为双系统结构以支持高频动作输出。
- 当前方法仅使用基于MPC的路点控制器，缺乏更灵活的运动模型，未来可集成更多运动能力。



---


**注**：本文为个人学习笔记，大量内容来自网络公开资料，仅供参考。如有错误或建议，欢迎指正！
