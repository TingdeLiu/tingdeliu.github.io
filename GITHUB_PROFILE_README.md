# ğŸ¤– Embodied AI Engineer | Vision-Language Navigation Specialist

```
 _____ _                 _        _     _       
|_   _(_)_ __   __ _  __| | ___  | |   (_)_   _ 
  | | | | '_ \ / _` |/ _` |/ _ \ | |   | | | | |
  | | | | | | | (_| | (_| |  __/ | |___| | |_| |
  |_| |_|_| |_|\__, |\__,_|\___| |_____|_|\__,_|
               |___/                             
```

<div align="center">

[![Profile Views](https://komarev.com/ghpvc/?username=TingdeLiu&color=blueviolet&style=flat-square&label=Profile+Views)](https://github.com/TingdeLiu)
[![GitHub followers](https://img.shields.io/github/followers/TingdeLiu?style=social)](https://github.com/TingdeLiu?tab=followers)
[![Website](https://img.shields.io/website?down_color=red&down_message=offline&up_color=green&up_message=online&url=https%3A%2F%2Ftingdeliu.github.io)](https://tingdeliu.github.io/)

### ğŸ—£ï¸ Vision-Language Navigation â€¢ ğŸ¤– Embodied AI â€¢ ğŸ¯ VLA Models

</div>

---

## ğŸ‘¨â€ğŸ’» About Me

```python
class EmbodiedAIEngineer:
    def __init__(self):
        self.name = "Tingde Liu"
        self.role = "Embodied AI Engineer | VLN Specialist"
        self.location = "Hannover, Germany ğŸ‡©ğŸ‡ª"
        self.education = "M.Sc. Robotics @ Leibniz UniversitÃ¤t Hannover"
        self.research_focus = [
            "Vision-Language Navigation (VLN)",
            "Embodied AI & Spatial Intelligence",
            "Vision-Language-Action (VLA) Models",
            "3D Scene Understanding & SLAM",
            "LLM for Robotic Reasoning"
        ]
    
    def get_current_focus(self):
        return {
            "primary": "Teaching robots to navigate using natural language",
            "goal": "Bridge language understanding and spatial action",
            "approach": "Multi-modal learning (Vision + Language + Action)"
        }
    
    def say_hi(self):
        print("Hi! Let's build intelligent agents that see, understand, and act! ğŸš€")

me = EmbodiedAIEngineer()
me.say_hi()
```

## ğŸ”¬ Research Interests

<table>
<tr>
<td width="50%">

### Core Focus
- ğŸ—ºï¸ **Vision-Language Navigation (VLN)**
  - Language-guided robot navigation
  - Instruction following in 3D environments
  - Cross-modal learning & grounding
  
- ğŸ¤– **Embodied AI**
  - Agents in physical/virtual worlds
  - Spatial reasoning & understanding
  - Interactive learning

</td>
<td width="50%">

### Technical Areas
- ğŸ¯ **Vision-Language-Action (VLA)**
  - End-to-end multimodal control
  - Policy learning for manipulation
  
- ğŸ“ **SLAM & 3D Vision**
  - Point cloud processing
  - Semantic mapping
  - Depth estimation

- ğŸ§  **LLM for Robotics**
  - Foundation models for spatial tasks
  - Multi-modal reasoning

</td>
</tr>
</table>

## ğŸ’» Tech Stack

<div align="center">

### ğŸš€ Core Technologies

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![C++](https://img.shields.io/badge/C++-00599C?style=for-the-badge&logo=cplusplus&logoColor=white)

### ğŸ¤– AI & Robotics

![Transformers](https://img.shields.io/badge/ğŸ¤—_Transformers-FFD21E?style=for-the-badge)
![ROS](https://img.shields.io/badge/ROS-22314E?style=for-the-badge&logo=ros&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-76B900?style=for-the-badge&logo=nvidia&logoColor=white)

### ğŸ› ï¸ Development Tools

![Git](https://img.shields.io/badge/Git-F05032?style=for-the-badge&logo=git&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Linux](https://img.shields.io/badge/Linux-FCC624?style=for-the-badge&logo=linux&logoColor=black)
![VS Code](https://img.shields.io/badge/VS_Code-007ACC?style=for-the-badge&logo=visualstudiocode&logoColor=white)

</div>

## ğŸ“Š GitHub Stats

<div align="center">

<img height="180em" src="https://github-readme-stats.vercel.app/api?username=TingdeLiu&show_icons=true&theme=radical&hide_border=true&bg_color=0D1117&title_color=00FF00&icon_color=00FF00&text_color=FFFFFF&count_private=true" />

<img height="180em" src="https://github-readme-stats.vercel.app/api/top-langs/?username=TingdeLiu&layout=compact&theme=radical&hide_border=true&bg_color=0D1117&title_color=00FF00&text_color=FFFFFF" />

</div>

<div align="center">

[![GitHub Streak](https://github-readme-streak-stats.herokuapp.com/?user=TingdeLiu&theme=radical&hide_border=true&background=0D1117&stroke=00FF00&ring=00FF00&fire=FF6600&currStreakLabel=00FF00)](https://git.io/streak-stats)

</div>

## ğŸš€ Featured Projects

<div align="center">

<table>
<tr>
<td width="50%">

### ğŸ—ºï¸ VLN Research
**Vision-Language Navigation**
- Instruction-following agents
- Multi-modal learning pipelines
- 3D environment understanding

**Tech:** PyTorch, Transformers, Habitat-Sim

</td>
<td width="50%">

### ğŸ§  MMS-LLM
**3D Large Language Model**
- Spatial reasoning with LLMs
- Point cloud understanding
- Multi-modal fusion

**Institution:** Institute of Cartography & Geoinformatics, LUH

</td>
</tr>
<tr>
<td width="50%">

### ğŸ“ SLAM & Planning
**Localization & Path Planning**
- Real-time SLAM implementation
- Intelligent path planning
- Multi-sensor fusion

**Tech:** ROS, C++, PCL

</td>
<td width="50%">

### ğŸ” RAG Systems
**Retrieval-Augmented Generation**
- Document understanding
- Knowledge retrieval
- LLM applications

**Tech:** LangChain, Vector DB, Embeddings

</td>
</tr>
</table>

</div>

## ğŸ“ Latest Blog Posts

<!-- BLOG-POST-LIST:START -->
ğŸ¤– Check out my [research blog](https://tingdeliu.github.io/) for deep dives into:
- Vision-Language Navigation papers
- Embodied AI implementations
- SLAM & 3D vision tutorials
- LLM for robotics insights
<!-- BLOG-POST-LIST:END -->

## ğŸ“ˆ Activity Graph

<div align="center">

[![Tingde's github activity graph](https://github-readme-activity-graph.vercel.app/graph?username=TingdeLiu&theme=github-compact&hide_border=true&bg_color=0D1117&color=00FF00&line=00FF00&point=FFFFFF)](https://github.com/TingdeLiu)

</div>

## ğŸŒ Connect With Me

<div align="center">

```javascript
const contact = {
    email: "tingde.liu.ai@gmail.com",
    linkedin: "linkedin.com/in/tingde-liu-379818270",
    website: "tingdeliu.github.io",
    github: "github.com/TingdeLiu",
    location: "Hannover, Germany ğŸ‡©ğŸ‡ª"
};

console.log(`Let's collaborate on Embodied AI! ğŸ¤`);
```

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0A66C2?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/tingde-liu-379818270/)
[![Email](https://img.shields.io/badge/Email-EA4335?style=for-the-badge&logo=gmail&logoColor=white)](mailto:tingde.liu.ai@gmail.com)
[![Website](https://img.shields.io/badge/Website-4285F4?style=for-the-badge&logo=google-chrome&logoColor=white)](https://tingdeliu.github.io/)
[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/TingdeLiu)

</div>

---

<div align="center">

### ğŸ’¡ _"Teaching machines to see, understand, and navigate the world through language"_

```
if (robot.understand(language) && robot.perceive(environment)) {
    robot.navigate(goal);
    return "Success! ğŸ¯";
}
```

**æ¢ç´¢å…·èº«æ™ºèƒ½çš„æ— é™å¯èƒ½ | Exploring the Frontiers of Embodied AI**

![Wave](https://capsule-render.vercel.app/api?type=waving&color=gradient&customColorList=6,11,20&height=100&section=footer&text=Thanks%20for%20visiting!&fontSize=20&fontColor=fff&animation=twinkling)

</div>
